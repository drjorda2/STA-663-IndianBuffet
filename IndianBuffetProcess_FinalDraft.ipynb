{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Uploading required packages\n",
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import uniform as unif\n",
    "from scipy.stats import multivariate_normal as mvtnorm\n",
    "from scipy.stats import bernoulli\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> STA 663 Final Project <center>\n",
    "## <center>  The Indian Buffet Process and its applications <center> \n",
    "### <center> Sunith R. Suresh & Drew Jordan <center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "In un-supervised learning, the goal is to determine latent (hidden) from unlabeled data groups. In other words, we would like to group the data into categories, such that the categories are learned from the data itself. One way is to use a standard clustering algorithm like k-means or Gaussian mixture modeling. The problem is that both these methods assume a fixed number of groups that are determined by the user. Unfortunately, most real world data do not have a fixed number of groups. Non-parametric Bayesian methods allow us to model such data by allowing the number of groups to grow with the addition of more data. One such non-parametric method is the Chinese Restaurant Process, where we allocate a data point to a group with some probability while also allowing a data point to form a new group. The Indian Buffet Process is an extension of the Chinese Restaurant Process in which we allow data points to belong to multiple groups. For the purpose of this paper, we shall implement the algorithm described in the paper “Infinite Latent Feature Models and the Indian Buffet Process” by Griffiths and Ghahramani (2005). We will simulate an image dataset similar to that used in Griffiths and Ghahramani (2005) to test our code. Once we implement the algorithm, we shall attempt to optimize it in regards to computational efficiency. \n",
    "\n",
    "### 2. The Indian Buffet Process\n",
    "\n",
    "The Indian buffet process (IBP) is a stochastic process for defining the probability distribution over equivalence classes of sparse binary matrices with a finite number of rows and an unbounded number of columns. \n",
    "\n",
    "The IBP is best described with the help of a culinary metaphor. Customers (observations) sequentially arrive at an Indian buffet with a infinite number of dishes (features). The first customer samples $Possion(\\alpha)$ dishes. The $i^{th}$ customer arrives and samples the already sampled dishes with a probability proportional to the popularity of that dish prior to his arrival (i.e, proportional to $\\frac{n_{-i,k}}{i}$, where $n_{i,k}$ is the number of customer who sampled dish $k$ prior to the arrival of the $i^{th}$ customer). After the customer samples the already sampled dishes, he process to further sample another $Poisson(\\frac{\\alpha}{i})$ number of new dishes. This process continues till all $N$ customers have visited the buffet (Yildrim, 2012).\n",
    "\n",
    "The outcome of the Indian Buffet Process can be represented by a binary matrix $Z$ where the rows (denoted by $i$) are customers(observations) and the columns(denoted by k) are dishes(features). If the value in a particular row and columns is 1, it denotes that observation $i$ possesses feature $k$. \n",
    "\n",
    "For better intuition over the $\\alpha$ parameter, below are three matrices generated from IBP for varying values of $\\alpha ={2, 4, 6}$ with number of observation set to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indian Buffet Process Function\n",
    "\n",
    "def sampleIBP(alpha, num_objects):  \n",
    "    # Initializing storage for results\n",
    "    result = np.zeros([num_objects, 1000])\n",
    "    # Draw from the prior for alpha\n",
    "    t = np.random.poisson(alpha)\n",
    "    # Filling in first row of result matrix\n",
    "    result[0, 0:t] = np.ones(t) #changed form np.ones([1, t])\n",
    "    # Initializing K+\n",
    "    K_plus = t\n",
    "    \n",
    "    for i in range(1, num_objects):\n",
    "        for j in range(0, K_plus):\n",
    "            p = np.array([np.log(np.sum(result[0:i,j])) - np.log(i+1), \n",
    "                          np.log(i+1 - np.sum(result[0:i, j])) - np.log(i+1)])\n",
    "            p = np.exp(p - max(p))\n",
    "\n",
    "            if(np.random.uniform() < p[0]/np.sum(p)):\n",
    "                result[i, j] = 1\n",
    "            else:\n",
    "                result[i, j] = 0\n",
    "        t = np.random.poisson(alpha/(i+1))\n",
    "        x = K_plus + 1\n",
    "        y = K_plus + t\n",
    "        result[i, (x-1):y] = np.ones(t) #changed form np.ones([1, t])\n",
    "        K_plus = K_plus+t\n",
    "    result = result[:, 0:K_plus]\n",
    "    \n",
    "    return list([result, K_plus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWIAAADpCAYAAAAeXD3VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPpPBizwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c5efba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "[Z_alpha2,K1] = sampleIBP(2, 10)\n",
    "[Z_alpha3,K3] = sampleIBP(4, 10)\n",
    "[Z_alpha5,K5] = sampleIBP(6, 10)\n",
    "\n",
    "fig = plt.figure()\n",
    "a=fig.add_subplot(2,2,1)\n",
    "plt.imshow(Z_alpha2,cmap=plt.get_cmap('gray'),  interpolation='nearest')\n",
    "a=fig.add_subplot(2,2,2)\n",
    "plt.imshow(Z_alpha3,cmap=plt.get_cmap('gray'),  interpolation='nearest')\n",
    "a=fig.add_subplot(2,2,3)\n",
    "plt.imshow(Z_alpha5,cmap=plt.get_cmap('gray'),  interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have three images for the binary matrix Z generated form the Indian Buffet Process for $\\alpha ={2, 4, 6}$. In the above images, a white pixel represents a value of 1 while a black pixel represents a value of 0. $\\alpha$ can be viewed as a concentration parameter. For equal number of observation, $\\alpha$ influence how likely it is that multiple observations share the same feature.\n",
    "\n",
    "Two aspects of the Indian Buffet make it suitable for latent feature modeling (Yildrim, 2012)\n",
    "\n",
    "1. It defines a probability distribution over rich combinatoric structures (We can interpret binary matrices are feature ownership tables, adjacency matrices etc)\n",
    "\n",
    "2. A priori, IBP has support over binary matrices with any number of columns. Upon observing data, IBP concentrates its mass over a subset of binary matrices with finitely many columns via probabilitic inferences. Therefore IBP, allows binary matrices to grow and shrink with more data.\n",
    "\n",
    "The IBP does assumes that latent features are binary, so an observation either does or does not possess a feature. Also it assumes that latent features are statistically independent, so knowledge that an observation possess a certain feature does not provide information about whether it possesses other features (Yildrim, 2012)\n",
    "\n",
    "Formally, the probability of a binary matrix, $Z \\sim IBP(\\alpha)$ is given by\n",
    "\n",
    "\n",
    "$$P(Z| \\alpha) = \\frac{\\alpha^{K}}{\\prod_{h=1}^{2^N - 1} K_{h}!} exp \\{ - \\alpha H_{N}\\} \\prod_{k=1}^{K} \\frac{(N- m_k)! (m_k - 1)!}{N!}$$\n",
    "\n",
    "$N$ - Number of objects\n",
    "\n",
    "$K$ - Number of features\n",
    "\n",
    "$K_h$ - Number of features with history h \n",
    "\n",
    "$H_N$ - $N^{th}$ Harmonic number\n",
    "\n",
    "$m_k$ - Number of objects with feature k\n",
    "\n",
    "$\\alpha$ - parameter influecing the Indian Buffet Process\n",
    "\n",
    "A detailed derivation of the equation can be found in Griffiths & Ghahramani, 2005.\n",
    "\n",
    "\n",
    "The conditional distribution can be given by\n",
    "\n",
    "$$ P( z_{i,k} = 1 \\mid z_{-i,k}) = \\frac{n_{-i, k}}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Application of the Indian Buffet Process for Unsupervised Latent Feature Learning\n",
    "\n",
    "### 3.1 Linear Gaussian Model\n",
    "\n",
    "We shall demonstrate the application of the IBP, by using it a prior for unsupervised latent feature learning. We shall consider a simple linear-Gaussian model described by Griffiths & Ghahramani (Griffiths & Ghahramani, 2005).\n",
    "\n",
    "Let $Z$ be a binary feature ownership matrix, where $z_{i,k}=1$ indicating that object $i$ posssess feature $k$\n",
    "\n",
    "Lets assume $Z \\sim IBP(\\alpha)$\n",
    "\n",
    "Let X be real-valued observation, where $x_{i,j}$ is the value of feature $j$ for object $i$. $X$ is a ${N x D}$ matrixs.\n",
    "\n",
    "Let $z_i$ be features values for object $i$ and let $x_i$ be the observed values for object $i$.  Using a linear gaussian model\n",
    "\n",
    "$$x_i \\sim N(z_i W, \\sigma_x^2 I) $$\n",
    "\n",
    "where $W$ is weight matrix is a $\\textit{K x D}$ matrix of weights representing the $\\textit{K}$ latent features,\n",
    "\n",
    "$$W \\sim N(0, \\sigma_w^2 I)$$\n",
    "\n",
    "The likelihood is given by\n",
    "\n",
    "$$p( X \\mid Z, W, \\sigma_x^2) = \\frac{1}{(2 \\pi \\sigma_x^2)^{\\frac{N D}{2}}} exp\\{ -\\frac{1}{2 \\sigma_x^2} tr((X-ZW)^{T} (X-ZW)^{T}))\\} $$\n",
    "\n",
    "\n",
    "Marginalizing out $W$\n",
    "\n",
    "$$P(X|Z,\\sigma_X, \\sigma_A) = \\frac{1}{(2 \\pi)^{ND/2} (\\sigma_X)^{(N-K)D}(\\sigma_A)^{KD}(|Z^TZ+\\frac{\\sigma_X^2}{\\sigma_A^2}I|)^{D/2}} exp\\{-\\frac{1}{2\\sigma_X^2}tr(X^T(I-Z M Z^T)X)\\}$$\n",
    "\n",
    "where $$ M = (Z^TZ+\\frac{\\sigma_X^2}{\\sigma_A^2}I)^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inference with MCMC Sampling Methods  \n",
    "\n",
    "Exact inference is intractable and instead, we can get approximate inference using Monte Carlo Markov Chain (MCMC) sampling methods. The MCMC method we use is proposed by Griffiths and Ghahramani (Griffiths and Ghahramani, 2005)\n",
    "\n",
    "#### Parameters of Interest\n",
    "\n",
    "We have five parameters of interest in our model that we need to update throughout our MCMC process.\n",
    "\n",
    "1.) Z: feature ownernship matrix\n",
    "\n",
    "2.) $K_+$: number of new latent features\n",
    "\n",
    "3.) $\\alpha$ parameter controlling  $K_+$\n",
    "\n",
    "4.) $\\sigma_x$\n",
    "\n",
    "5.) $\\sigma_A$\n",
    "\n",
    "We are able to find known full conditional distributions for Z, $K_+$, and $\\alpha$, so we will update them using Gibbs Sampling. We will update $\\sigma_X$ and $\\sigma_A$ using random-walk Metropolis-Hastings.\n",
    "\n",
    "#### Prior Distributions\n",
    "\n",
    "The priors we used are\n",
    "\n",
    "$$P(z_{ik} = 1 | \\textbf{z}_{-i,k}) = \\dfrac{n_{-i,k}}{N} $$\n",
    "\n",
    "$$\\alpha \\sim Gamma(1,1)$$\n",
    "\n",
    "$$K_+ \\sim Poisson(\\dfrac{\\alpha}{N})$$\n",
    "\n",
    "\n",
    "#### Gibbs Sampling Updates\n",
    "\n",
    "For observation $i$ with more than one feature, we can sample $z_{i,k}$ using the full conditional distribution.\n",
    "\n",
    "$$P(z_{ik}|X,Z_{-(i,k),},\\sigma_X,\\sigma_A) \\propto  P(X|Z,\\sigma_X, \\sigma_A) * P(z_{ik}=1|\\textbf{z}_{-i,k})$$\n",
    "\n",
    "Sample new features for observation $i$ by first computing a truncated distribution for $K_+$ using the data likelihood and the  prior for $K_+$ up to 4 new features, and then sampling from the truncated distribution. \n",
    "\n",
    "Sample $P(\\alpha \\mid Z) \\sim Gamma(1 + K_+, 1 + \\sum_{i=1}^{N} H_i)$\n",
    "\n",
    "#### Metropolis-Hasting Updates\n",
    "\n",
    "To update $\\sigma_X$ and $\\sigma_A$, we use random walk Metropolis-Hastings steps. For $\\sigma_X$, we generate a random value from a Uniform(-.05, .05) distribution and add this value to our current value of $\\sigma_X$ to get $\\sigma_X^*$. We then accept our new value of $\\sigma_X$ with probability:\n",
    "\n",
    "$$p = min(1, \\dfrac{P(X|Z, \\sigma_X^*, \\sigma_A}{P(X|Z, \\sigma_X, \\sigma_A})$$\n",
    "\n",
    "To update $\\sigma_A$, we follow the same proceedure as with $\\sigma_X$, replacing $\\sigma_X$ with $\\sigma_A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. A Demonstration with Simulated Data\n",
    "\n",
    "#### 4.1 Latent Features \n",
    "\n",
    "For a demonstration of the algorithim, we used a synthetic data proposed by Yildrim (Yildrim, 2012). Four base images were created consisting of ${6 x 6}$ pixels. Each base image can be represented by a vector of length $36 \\big( 6* 6 \\big)$. Theses base images represent the latent features and all the latent features by represented by a $K x D$ weight matrix $W$ where $K=4$ and $D=36$\n",
    "\n",
    "The four base (latent) images are shown below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAABlCAYAAACRByIyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c5689b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Data Simulation\n",
    "\n",
    "#Latent Features\n",
    "W = np.array([[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]])\n",
    "\n",
    "##Latent Features\n",
    "fig = plt.figure()\n",
    "a=fig.add_subplot(1,4,1)\n",
    "plt.imshow(np.matrix(W[0]).reshape(6,6),cmap=plt.get_cmap('gray'),  interpolation='nearest',origin='lower')\n",
    "a=fig.add_subplot(1,4,2)\n",
    "plt.imshow(np.matrix(W[1]).reshape(6,6),cmap=plt.get_cmap('gray'),  interpolation='nearest',origin='lower')\n",
    "a=fig.add_subplot(1,4,3)\n",
    "plt.imshow(np.matrix(W[2]).reshape(6,6),cmap=plt.get_cmap('gray'),  interpolation='nearest',origin='lower')\n",
    "a=fig.add_subplot(1,4,4)\n",
    "plt.imshow(np.matrix(W[3]).reshape(6,6),cmap=plt.get_cmap('gray'),  interpolation='nearest',origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Simulated Images\n",
    "\n",
    "We then created 100 synthetic images, where each image $x_i$ is superposition of zero or more base images (latent feature) with added white noise. $X$ is a $100 x D$ matrix\n",
    "\n",
    "$$x_i \\sim N(z_i W, \\sigma_x^2 I) $$\n",
    "\n",
    "Here, $z_i$ represents a row of a binary feature matrix $Z$ of dimension $100 x K$. A values in $z_i$ is 1 or 0 with a probability of 0.5. A value of 1 in $z_i$ corresponds to image $x_i$ containing the correspoing base image. $\\sigma_x^2$ which controls the white noise is set to 0.5\n",
    "\n",
    "Below is an example of one such simulated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Each image in our simulated data set is the superposition of four base images#\n",
    "# Number of images/ data points\n",
    "num_objects=100\n",
    "\n",
    "#Dimension of image (6x6)\n",
    "object_dim  = 6*6\n",
    "\n",
    "#Covariance matrix for images/ white noise\n",
    "sigma_x_orig = 0.5\n",
    "I = sigma_x_orig * np.identity(object_dim)\n",
    "\n",
    "#z_i - binary feature matrix (1 x 4) - each entry set to 1 with probability 0.5 and 0 otherwise#\n",
    "#x is data variable - each row correspondes to a superimposed built from a random combination of latent features#\n",
    "#with white noise added - x is built with multivariate gaussian#\n",
    "image_data = np.zeros((100,36))\n",
    "z_org = np.zeros((100,4))\n",
    "\n",
    "for i in range(0,num_objects):\n",
    "    z_org[i,:] = np.array([bernoulli.rvs(p=0.5, size=4)])\n",
    "    image_data[i,:] = np.dot(z_org[i,:],W) + np.random.normal(0,1, (1,object_dim)).dot(I) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10c584eb8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAABlCAYAAACRByIyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUucsIpIgdc4iIglS5ywikqD/A9gtzcbTS/QrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1098b9898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Simulated Data\n",
    "fig = plt.figure()\n",
    "a=fig.add_subplot(1,4,1)\n",
    "plt.imshow(np.dot(z_org[1,0],W)[0,:].reshape(6,6),cmap=plt.get_cmap('gray'),  interpolation='nearest',origin='lower')\n",
    "a=fig.add_subplot(1,4,2)\n",
    "plt.imshow(np.dot(z_org[1,0],W)[1,:].reshape(6,6),cmap=plt.get_cmap('gray'),  interpolation='nearest',origin='lower')\n",
    "a=fig.add_subplot(1,4,3)\n",
    "plt.imshow(np.dot(z_org[1,0],W)[3,:].reshape(6,6),cmap=plt.get_cmap('gray'),  interpolation='nearest',origin='lower')\n",
    "a=fig.add_subplot(1,4,4)\n",
    "plt.imshow(np.matrix(image_data[1,:]).reshape(6,6),cmap=plt.get_cmap('gray'),  interpolation='nearest',origin='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_1$ in our simulated data set is a superimposition of base images(latent features) 1,2 and 4 with added white noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Profiling & Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Original MCMC sampler\n",
    "\n",
    "We wrote down the MCMC sampler as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Helper Functions'''\n",
    "\n",
    "# This function return the log likelihood\n",
    "def likelihood(X, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim):\n",
    "    part1 = (-1)*num_objects*(0.5*object_dim)*np.log(2*np.pi)\n",
    "    part2 = (-1)*(num_objects-K_plus)* object_dim *np.log(sigma_X) \n",
    "    part3 = (-1)*object_dim*K_plus*np.log(sigma_A) \n",
    "    part4 = (-1)*(0.5*object_dim)* np.log(np.linalg.det((np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2)*np.eye(K_plus)))) \n",
    "    part5 = (-1/(2*sigma_X**2)) * np.trace(np.dot(np.dot(X.T,(np.identity(num_objects) - np.dot(np.dot(Z,M),Z.T))),X))\n",
    "    total = part1+part2+part3+part4+part5\n",
    "    return(total)\n",
    "\n",
    "#This function computes matrix M\n",
    "def Mcalc(Z, sigma_X, sigma_A, K_plus):\n",
    "    M = np.linalg.inv(np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2) * np.eye(K_plus))\n",
    "    return(M)\n",
    "\n",
    "#This function samples new value of Z[i,k] using Gibbs Sampling\n",
    "def Met_zval(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim, i, k):    \n",
    "    \n",
    "    P=np.zeros(2)\n",
    "\n",
    "    Z[i,k]=1\n",
    "    #Compute posterior density of new_sample\n",
    "    M = Mcalc(Z, sigma_X, sigma_A, K_plus)\n",
    "    P[0] = likelihood(data, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(np.sum(Z[:, k]) - Z[i,k]) - np.log(num_objects)\n",
    "\n",
    "    #Set new_sample to 0\n",
    "    Z[i,k]=0\n",
    "    #Computer posterior density of new_sample\n",
    "    M = Mcalc(Z, sigma_X, sigma_A, K_plus)\n",
    "    P[1] = likelihood(data, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(num_objects - np.sum(Z[:,k])) - np.log(num_objects)\n",
    "\n",
    "    P = np.exp(P - np.max(P))\n",
    "\n",
    "    if np.random.uniform(0,1) < (P[0]/(np.sum(P))):\n",
    "        new_sample = 1\n",
    "    else:\n",
    "        new_sample = 0\n",
    "    \n",
    "    return(new_sample)\n",
    "\n",
    "\n",
    "#This function samples new dishes upto a range specified by trunc_val\n",
    "def New_dishes(data, Z, sigma_X, sigma_A, K_plus, alpha, num_objects, object_dim, trunc_val,i):\n",
    "    trunc = np.zeros(trunc_val)\n",
    "    alpha_N = alpha/num_objects\n",
    "\n",
    "    for k_i in range(0,trunc_val):\n",
    "        Z_temp = Z\n",
    "        if k_i>0:\n",
    "            newcol = np.zeros((num_objects, k_i))\n",
    "            newcol[i,:] = 1 \n",
    "            Z_temp = np.column_stack((Z_temp, newcol))\n",
    "        M = Mcalc(Z_temp, sigma_X, sigma_A, K_plus+k_i)\n",
    "        trunc[k_i] = k_i * np.log(alpha_N) - alpha_N - np.log(np.math.factorial(k_i)) + likelihood(data, Z_temp, M, sigma_A, sigma_X, K_plus+k_i, num_objects, object_dim)\n",
    "\n",
    "    trunc = np.exp(trunc - np.max(trunc))\n",
    "    trunc = trunc/np.sum(trunc)\n",
    "\n",
    "    p = np.random.uniform(0,1)\n",
    "    t = 0\n",
    "    new_dishes = 0\n",
    "\n",
    "    for k_i in range(0,trunc_val):\n",
    "        t = t + trunc[k_i]\n",
    "        if p < t:\n",
    "            new_dishes = k_i\n",
    "            break\n",
    "            \n",
    "    return(new_dishes)\n",
    "\n",
    "#The function updates both sigma_X and sigma_A using random-walk Metropolis-Hastings.\n",
    "def Met_sigma(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim):\n",
    "    \n",
    "    M = Mcalc(Z, sigma_X, sigma_A, K_plus)  \n",
    "    lik_curr = likelihood(data, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim)\n",
    "\n",
    "    if np.random.uniform(0,1) < 0.5:\n",
    "        sigma_X_new = sigma_X - np.random.uniform(0,1)/20\n",
    "    else:\n",
    "        sigma_X_new = sigma_X + np.random.uniform(0,1)/20\n",
    "\n",
    "    M = Mcalc(Z, sigma_X_new, sigma_A, K_plus)\n",
    "    lik_new_X = likelihood(data, Z, M, sigma_A, sigma_X_new, K_plus, num_objects, object_dim)\n",
    "\n",
    "    acc_X = np.exp(min(0, lik_new_X - lik_curr))\n",
    "\n",
    "    if np.random.uniform(0,1) < 0.5:\n",
    "        sigma_A_new = sigma_A - np.random.uniform(0,1)/20\n",
    "    else:\n",
    "        sigma_A_new = sigma_A + np.random.uniform(0,1)/20\n",
    "\n",
    "    M = Mcalc(Z, sigma_X, sigma_A_new, K_plus)\n",
    "    lik_new_A = likelihood(data, Z, M, sigma_A_new, sigma_X, K_plus, num_objects, object_dim)\n",
    "\n",
    "    acc_A = np.exp(min(0, lik_new_A - lik_curr))\n",
    "    \n",
    "    sigma_X_val=0\n",
    "    sigma_A_val=0\n",
    "\n",
    "    if np.random.uniform(0,1) < acc_X:\n",
    "        sigma_X_val = sigma_X_new\n",
    "    else:\n",
    "        sigma_X_val = sigma_X\n",
    "    \n",
    "    if np.random.uniform(0,1) < acc_A:\n",
    "        sigma_A_val = sigma_A_new\n",
    "    else:\n",
    "        sigma_A_val = sigma_A\n",
    "        \n",
    "    return list([sigma_X_val, sigma_A_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''The MCMC Sampler'''\n",
    "\n",
    "def Sampler(data, num_objects, object_dim, E=1000,  K_inf = 20, sigma_X = 1, sigma_A = 1, alpha = 1, trunc_val=5):\n",
    "    #Set storage arrays for sampled parameters\n",
    "    chain_Z = np.zeros([E, num_objects, K_inf])\n",
    "    chain_K = np.zeros([E, 1])\n",
    "    chain_sigma_X = np.zeros([E, 1])\n",
    "    chain_sigma_A = np.zeros([E, 1])\n",
    "    chain_alpha = np.zeros([E, 1])\n",
    "\n",
    "    #Initialize parameter values\n",
    "    num_object= np.shape(data)[0]\n",
    "    object_dim = np.shape(data)[1]\n",
    "    [Z, K_plus] = sampleIBP(alpha, num_objects)\n",
    "\n",
    "    #Compute Harmonic Number\n",
    "    HN = 0\n",
    "    for i in range(0, num_objects):\n",
    "        HN = HN + 1.0/(i+1)\n",
    "\n",
    "    for e in range(0, E):\n",
    "        #Store sampled values\n",
    "        chain_Z[e, :, 0:K_plus] = Z[:, 0:K_plus]\n",
    "        chain_K[e] = K_plus\n",
    "        chain_sigma_X[e] = sigma_X\n",
    "        chain_sigma_A[e] = sigma_A\n",
    "        chain_alpha[e] = alpha\n",
    "\n",
    "        #if (e%100==0):\n",
    "        #    print(e)\n",
    "        #print(\"At iteration\", e, \": K_plus is\", K_plus, \", alpha is\", alpha) \n",
    "\n",
    "        #Generate a new value for Z[i,k] and accept by Metropolis\n",
    "        for i in range(0, num_objects):\n",
    "            #First we remove singular features if any\n",
    "            for k in range(0, K_plus):\n",
    "                if (k>=K_plus):\n",
    "                    break\n",
    "                if(Z[i, k] > 0):\n",
    "                    if (np.sum(Z[:, k]) - Z[i, k]) <= 0: \n",
    "                        Z[i, k] = 0\n",
    "                        Z[:, k:(K_plus - 1)] = Z[:, (k+1):K_plus]\n",
    "                        K_plus = K_plus - 1\n",
    "                        Z = Z[:, 0:K_plus]\n",
    "                        continue\n",
    "                #Sample new values fo z_ik\n",
    "                Z[i,k] = Met_zval(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim, i, k)\n",
    "\n",
    "            #Sample new dishes \n",
    "            new_dishes = New_dishes(data, Z, sigma_X, sigma_A, K_plus, alpha, num_objects, object_dim, trunc_val,i)\n",
    "\n",
    "            if(new_dishes > 0):\n",
    "                newcol = np.zeros((num_objects, new_dishes))\n",
    "                newcol[i,:] = 1\n",
    "                Z = np.column_stack((Z, newcol))\n",
    "            K_plus = K_plus + new_dishes\n",
    "\n",
    "        #Sample sigma_X and sigma_A through Metropolis\n",
    "        [sigma_X, sigma_A] = Met_sigma(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim)\n",
    "        \n",
    "        #Sample alpha via Gibbs\n",
    "        alpha = np.random.gamma(1 + K_plus, 1/(1+HN))\n",
    "    \n",
    "    print(\"Complete\")\n",
    "    return list([chain_Z, chain_K, chain_sigma_X, chain_sigma_A, chain_alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling Our Sampler\n",
    "### 5.2 Profiling Our Sampler\n",
    "In order to test the efficiency our code, we profile our Sampler function to find any potential bottlenecks in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n",
      "Fri Apr 29 17:21:49 2016    profiling_unoptimized\n",
      "\n",
      "         9773048 function calls in 66.686 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 65 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   66.686   66.686 {built-in method builtins.exec}\n",
      "        1    0.000    0.000   66.686   66.686 <string>:1(<module>)\n",
      "        1    0.290    0.290   66.686   66.686 <ipython-input-9-96d59dce4ebe>:1(Sampler)\n",
      "   133080    9.179    0.000   47.531    0.000 <ipython-input-4-cf0b3993be3a>:2(likelihood)\n",
      "    41390    1.970    0.000   41.285    0.001 <ipython-input-6-7997664e2c7c>:2(Met_zval)\n",
      "   798480   26.625    0.000   26.625    0.000 {built-in method numpy.core.multiarray.dot}\n",
      "    10000    1.440    0.000   24.778    0.002 <ipython-input-7-3b4495a7a593>:1(New_dishes)\n",
      "   133080    2.092    0.000   12.381    0.000 <ipython-input-5-c62396fa41c4>:1(Mcalc)\n",
      "   133080    4.295    0.000    7.242    0.000 linalg.py:458(inv)\n",
      "   133080    2.753    0.000    5.417    0.000 linalg.py:1723(det)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Profiling our code for 100 iterations of our Sampler Function\n",
    "import cProfile\n",
    "import re\n",
    "np.random.seed(1234)\n",
    "cProfile.run('Sampler(image_data, num_objects, object_dim, E=100,  K_inf = 20, sigma_X = 1, sigma_A = 1, alpha = 1, trunc_val=5)', \"profiling_unoptimized\")\n",
    "\n",
    "# Displaying results of our profiling\n",
    "import pstats\n",
    "p = pstats.Stats('profiling_unoptimized')\n",
    "p.strip_dirs().sort_stats(\"cumulative\").print_stats(10)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from our profiling output above that the biggest bottleneck within our code is our likelihood function. We noticed that we call the Mcalc function along with likelihood function in tandem several times throughout our Sampler function. To reduce the number of calls and to speed up our Sampler function, we decided to move the calculation of the M matrix into our likelihood function. \n",
    "\n",
    "Another optimization technique we considered was using a technique decribed by Griffiths and Ghahramani (Griffiths and Ghahramani, 2005) that calculated matrix M using only rank 1 updates to invert the matrix. However they mentioned that the procedure can lead to the accumulation of numerical errors and a full rank update would be required ocassionally. We decided not to implement this technique as our sampler was not unbearably slow, and the method described would unecessarily complicate the sampler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Optimizing the likelihood function\n",
    "\n",
    "We optimzed the liklelihood function by calculating the M matrix within our likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Optimized Likelihood & Helper Functions'''\n",
    "\n",
    "# This function return the log likelihood\n",
    "def likelihood_opt(X, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim):\n",
    "    #Calculate M\n",
    "    M = np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2)*np.eye(K_plus)\n",
    "    \n",
    "    part1 = (-1)*num_objects*(0.5*object_dim)*np.log(2*np.pi)\n",
    "    part2 = (-1)*(num_objects-K_plus)* object_dim *np.log(sigma_X) \n",
    "    part3 = (-1)*object_dim*K_plus*np.log(sigma_A) \n",
    "    part4 = (-1)*(0.5*object_dim)* np.log(np.linalg.det(M)) \n",
    "    part5 = (-1/(2*sigma_X**2)) * np.trace(np.dot(np.dot(X.T,(np.identity(num_objects) - np.dot(np.dot(Z,np.linalg.inv(M)),Z.T))),X))\n",
    "    total = part1+part2+part3+part4+part5\n",
    "    return(total)\n",
    "\n",
    "\n",
    "\n",
    "#This function samples new value of Z[i,k]\n",
    "def Met_zval_opt(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim, i, k):    \n",
    "    \n",
    "    P=np.zeros(2)\n",
    "\n",
    "    Z[i,k]=1\n",
    "    #Compute posterior density of new_sample\n",
    "    P[0] = likelihood_opt(data, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(np.sum(Z[:, k]) - Z[i,k]) - np.log(num_objects)\n",
    "\n",
    "    #Set new_sample to 0\n",
    "    Z[i,k]=0\n",
    "    #Computer posterior density of new_sample\n",
    "    P[1] = likelihood_opt(data, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(num_objects - np.sum(Z[:,k])) - np.log(num_objects)\n",
    "\n",
    "    P = np.exp(P - np.max(P))\n",
    "\n",
    "    if np.random.uniform(0,1) < (P[0]/(np.sum(P))):\n",
    "        new_sample = 1\n",
    "    else:\n",
    "        new_sample = 0\n",
    "    \n",
    "    return(new_sample)\n",
    "\n",
    "#This function samples new dishes upto a range specified by trunc_val\n",
    "def New_dishes_opt(data, Z, sigma_X, sigma_A, K_plus, alpha, num_objects, object_dim, trunc_val,i):\n",
    "    trunc = np.zeros(trunc_val)\n",
    "    alpha_N = alpha/num_objects\n",
    "\n",
    "    for k_i in range(0,trunc_val):\n",
    "        Z_temp = Z\n",
    "        if k_i>0:\n",
    "            newcol = np.zeros((num_objects, k_i))\n",
    "            newcol[i,:] = 1 \n",
    "            Z_temp = np.column_stack((Z_temp, newcol))\n",
    "        trunc[k_i] = k_i * np.log(alpha_N) - alpha_N - np.log(np.math.factorial(k_i)) + likelihood_opt(data, Z_temp, sigma_A, sigma_X, K_plus+k_i, num_objects, object_dim)\n",
    "\n",
    "    trunc = np.exp(trunc - np.max(trunc))\n",
    "    trunc = trunc/np.sum(trunc)\n",
    "\n",
    "    p = np.random.uniform(0,1)\n",
    "    t = 0\n",
    "    new_dishes = 0\n",
    "\n",
    "    for k_i in range(0,trunc_val):\n",
    "        t = t + trunc[k_i]\n",
    "        if p < t:\n",
    "            new_dishes = k_i\n",
    "            break\n",
    "            \n",
    "    return(new_dishes)\n",
    "\n",
    "#The function updates both sigma_X and sigma_A using random-walk Metropolis-Hastings.\n",
    "def Met_sigma_opt(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim):\n",
    "     \n",
    "    lik_curr = likelihood_opt(data, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim)\n",
    "\n",
    "    if np.random.uniform(0,1) < 0.5:\n",
    "        sigma_X_new = sigma_X - np.random.uniform(0,1)/20\n",
    "    else:\n",
    "        sigma_X_new = sigma_X + np.random.uniform(0,1)/20\n",
    "\n",
    "    lik_new_X = likelihood_opt(data, Z, sigma_A, sigma_X_new, K_plus, num_objects, object_dim)\n",
    "\n",
    "    acc_X = np.exp(min(0, lik_new_X - lik_curr))\n",
    "\n",
    "    if np.random.uniform(0,1) < 0.5:\n",
    "        sigma_A_new = sigma_A - np.random.uniform(0,1)/20\n",
    "    else:\n",
    "        sigma_A_new = sigma_A + np.random.uniform(0,1)/20\n",
    "\n",
    "    lik_new_A = likelihood_opt(data, Z, sigma_A_new, sigma_X, K_plus, num_objects, object_dim)\n",
    "\n",
    "    acc_A = np.exp(min(0, lik_new_A - lik_curr))\n",
    "    \n",
    "    sigma_X_val=0\n",
    "    sigma_A_val=0\n",
    "\n",
    "    if np.random.uniform(0,1) < acc_X:\n",
    "        sigma_X_val = sigma_X_new\n",
    "    else:\n",
    "        sigma_X_val = sigma_X\n",
    "    \n",
    "    if np.random.uniform(0,1) < acc_A:\n",
    "        sigma_A_val = sigma_A_new\n",
    "    else:\n",
    "        sigma_A_val = sigma_A\n",
    "        \n",
    "    return list([sigma_X_val, sigma_A_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Performance of Optimized  Likelihood Function\n",
    "\n",
    "We now compare our optimized likelihood function likelihood_opt to our unoptimized version to see if we get any significant speed up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original Likelihood</th>\n",
       "      <td>0.00058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Opimized Likelihood</th>\n",
       "      <td>0.00041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Time\n",
       "Original Likelihood  0.00058\n",
       "Opimized Likelihood  0.00041"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Setting values to use in likelihood function comparisons\n",
    "num_objects = image_data.shape[0]\n",
    "object_dim = image_data.shape[1]\n",
    "\n",
    "sigma_X=1\n",
    "sigma_A=1\n",
    "alpha=1\n",
    "\n",
    "Z, K_plus = sampleIBP(alpha,num_objects)\n",
    "\n",
    "# Time the original likelihood function\n",
    "loops = 1000\n",
    "time_likelihood=np.zeros(loops)\n",
    "for l in range(loops):\n",
    "    t0=time.time()\n",
    "    M = Mcalc(Z, sigma_X, sigma_A, K_plus)\n",
    "    likelihood(image_data, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim)\n",
    "    t1=time.time()\n",
    "    time_likelihood[l]=t1-t0\n",
    "mean_time_likelihood= round(np.mean(time_likelihood),7)\n",
    "\n",
    "\n",
    "# Time the optimized likelihood function\n",
    "time_likelihood_opt = np.zeros(loops)\n",
    "for l in range(loops):\n",
    "    t0 = time.time()\n",
    "    likelihood_opt(image_data, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim)\n",
    "    t1 = time.time()\n",
    "    time_likelihood_opt[l] = t1-t0\n",
    "mean_time_likelihood_opt = round(np.mean(time_likelihood_opt), 7)\n",
    "\n",
    "time_array = np.array([mean_time_likelihood, mean_time_likelihood_opt])\n",
    "\n",
    "cols = [\"Time\"]\n",
    "index = [\"Original Likelihood\", \"Optimized Likelihood\"]\n",
    "\n",
    "time_df = pd.DataFrame(time_array, columns = cols, index = index)\n",
    "time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our optimized likelihood function produces a noticeable speed up over our original likelihood function. Over thousands of iterations, this small speed up should accumulate to produce a much faster sampler.\n",
    "\n",
    "We refactored our sampler to use the new optimized likelihood and helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampler_opt(data, num_objects, object_dim, E=1000,  K_inf = 20, sigma_X = 1, sigma_A = 1, alpha = 1, trunc_val=5):\n",
    "    #Set storage arrays for sampled parameters\n",
    "    chain_Z = np.zeros([E, num_objects, K_inf])\n",
    "    chain_K = np.zeros([E, 1])\n",
    "    chain_sigma_X = np.zeros([E, 1])\n",
    "    chain_sigma_A = np.zeros([E, 1])\n",
    "    chain_alpha = np.zeros([E, 1])\n",
    "\n",
    "    #Initialize parameter values\n",
    "    num_object= np.shape(data)[0]\n",
    "    object_dim = np.shape(data)[1]\n",
    "    [Z, K_plus] = sampleIBP(alpha, num_objects)\n",
    "\n",
    "    #Compute Harmonic Number\n",
    "    HN = 0\n",
    "    for i in range(0, num_objects):\n",
    "        HN = HN + 1.0/(i+1)\n",
    "\n",
    "    for e in range(0, E):\n",
    "        #Store sampled values\n",
    "        chain_Z[e, :, 0:K_plus] = Z[:, 0:K_plus]\n",
    "        chain_K[e] = K_plus\n",
    "        chain_sigma_X[e] = sigma_X\n",
    "        chain_sigma_A[e] = sigma_A\n",
    "        chain_alpha[e] = alpha\n",
    "\n",
    "        if (e%100==0):\n",
    "            print(e)\n",
    "        print(\"At iteration\", e, \": K_plus is\", K_plus, \", alpha is\", alpha) \n",
    "\n",
    "        #Generate a new value for Z[i,k] and accept by Metropolis\n",
    "        for i in range(0, num_objects):\n",
    "            #First we remove singular features if any\n",
    "            for k in range(0, K_plus):\n",
    "                if (k>=K_plus):\n",
    "                    break\n",
    "                if(Z[i, k] > 0):\n",
    "                    if (np.sum(Z[:, k]) - Z[i, k]) <= 0: \n",
    "                        Z[i, k] = 0\n",
    "                        Z[:, k:(K_plus - 1)] = Z[:, (k+1):K_plus]\n",
    "                        K_plus = K_plus - 1\n",
    "                        Z = Z[:, 0:K_plus]\n",
    "                        continue\n",
    "                #Compute conditional distribution for current cell\n",
    "                Z[i,k] = Met_zval_opt(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim, i, k)\n",
    "\n",
    "\n",
    "            #Sample new dishes by Metropolis\n",
    "            new_dishes = New_dishes_opt(data, Z, sigma_X, sigma_A, K_plus, alpha, num_objects, object_dim, trunc_val,i)\n",
    "\n",
    "            if(new_dishes > 0):\n",
    "                newcol = np.zeros((num_objects, new_dishes))\n",
    "                newcol[i,:] = 1\n",
    "                Z = np.column_stack((Z, newcol))\n",
    "            K_plus = K_plus + new_dishes\n",
    "\n",
    "        #Sample sigma_X and sigma_A through Metropolis\n",
    "        [sigma_X, sigma_A] = Met_sigma_opt(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim)\n",
    "        #Sample alpha via Gibbs\n",
    "        alpha = np.random.gamma(1 + K_plus, 1/(1+HN))\n",
    "    \n",
    "    print(\"Complete\")\n",
    "    return list([chain_Z, chain_K, chain_sigma_X, chain_sigma_A, chain_alpha])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
