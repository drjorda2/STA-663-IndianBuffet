{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Uploading required packages\n",
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import uniform as unif\n",
    "from scipy.stats import multivariate_normal as mvtnorm\n",
    "from scipy.stats import bernoulli\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indian Buffet Process Function\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indian Buffet Process Function\n",
    "\n",
    "def sampleIBP(alpha, num_objects):  \n",
    "    # Initializing storage for results\n",
    "    result = np.zeros([num_objects, 1000])\n",
    "    # Draw from the prior for alpha\n",
    "    t = np.random.poisson(alpha)\n",
    "    # Filling in first row of result matrix\n",
    "    result[0, 0:t] = np.ones(t) #changed form np.ones([1, t])\n",
    "    # Initializing K+\n",
    "    K_plus = t\n",
    "    \n",
    "    for i in range(1, num_objects):\n",
    "        for j in range(0, K_plus):\n",
    "            p = np.array([np.log(np.sum(result[0:i,j])) - np.log(i+1), \n",
    "                          np.log(i+1 - np.sum(result[0:i, j])) - np.log(i+1)])\n",
    "            p = np.exp(p - max(p))\n",
    "\n",
    "            if(np.random.uniform() < p[0]/np.sum(p)):\n",
    "                result[i, j] = 1\n",
    "            else:\n",
    "                result[i, j] = 0\n",
    "        t = np.random.poisson(alpha/(i+1))\n",
    "        x = K_plus + 1\n",
    "        y = K_plus + t\n",
    "        result[i, (x-1):y] = np.ones(t) #changed form np.ones([1, t])\n",
    "        K_plus = K_plus+t\n",
    "    result = result[:, 0:K_plus]\n",
    "    \n",
    "    return list([result, K_plus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Simulation\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Simulation\n",
    "\n",
    "#Latent Features\n",
    "W = np.array([[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]])\n",
    "\n",
    "\n",
    "#Each image in our simulated data set is the superposition of four base images#\n",
    "# Number of images/ data points\n",
    "num_objects=100\n",
    "\n",
    "#Dimension of image (6x6)\n",
    "object_dim  = 6*6\n",
    "\n",
    "#Covariance matrix for images/ white noise\n",
    "sigma_x_orig = 0.5\n",
    "I = sigma_x_orig * np.identity(object_dim)\n",
    "\n",
    "#z_i - binary feature matrix (1 x 4) - each entry set to 1 with probability 0.5 and 0 otherwise#\n",
    "#x is data variable - each row correspondes to a superimposed built from a random combination of latent features#\n",
    "#with white noise added - x is built with multivariate gaussian#\n",
    "image_data = np.zeros((100,36))\n",
    "z_org = np.zeros((100,4))\n",
    "\n",
    "for i in range(0,num_objects):\n",
    "    z_org[i,:] = np.array([bernoulli.rvs(p=0.5, size=4)])\n",
    "    image_data[i,:] = np.dot(z_org[i,:],W) + np.random.normal(0,1, (1,object_dim)).dot(I) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation\n",
    "====\n",
    "\n",
    "We will be using an infinite Gaussian binary latent feature model with an Indian Buffet Process prior to model the number of latent features present in our simulated image dataset. We follow the example as discussed in Griffiths and Ghahramani(2005).\n",
    "\n",
    "We begin by defining a binary feature ownership matrix $\\textbf{Z}$ that represents whether or not each feature is present given the observations $\\textbf{X}$. Therefore each D-dimensional object $\\textbf{x_i}$ has a Gaussian distribution:\n",
    "$$x_i \\sim Normal(z_i A, \\Sigma_X)$$\n",
    "\n",
    "Where $\\textbf{A}$ is a $\\textit{K x D}$ matrix of weights representing the $\\textit{K}$ latent features. The noise introduced into our simulated images is represented by the covariance $\\Sigma_X$. The prior on $\\textbf A$ is matrix Gaussian as well with mean 0 and covariance $\\Sigma_A$. We then integrate out $\\textbf A$ to generate the data likelihood:\n",
    "\n",
    "$$P(X|Z,\\sigma_X, \\sigma_A) = \\frac{1}{(2 \\pi)^{ND/2} (\\sigma_X)^{(N-K)D}(\\sigma_A)^{KD}(|Z^TZ+\\frac{\\sigma_X^2}{\\sigma_A^2}I|)^{D/2}} exp\\{-\\frac{1}{2\\sigma_X^2}tr(X^T(I-Z(Z^TZ+\\frac{\\sigma_X^2}{\\sigma_A^2}I)^{-1}Z^T)X)\\}$$\n",
    "\n",
    "### Gibbs Sampling and Metropolis-Hastings Algorithm\n",
    "\n",
    "We now implement both Gibbs Sampling and Metropolis-Hastings to perform inference on our image dataset.\n",
    "\n",
    "#### Parameters of Interest\n",
    "\n",
    "We have five parameters of interest in our model that we need to update throughout our MCMC process.\n",
    "\n",
    "1.) Z: feature ownernship matrix\n",
    "\n",
    "2.) $K_+$: number of new latent features\n",
    "\n",
    "3.) $\\alpha$ parameter for  $K_+$\n",
    "\n",
    "4.) $\\sigma_x$\n",
    "\n",
    "5.) $\\sigma_A$\n",
    "\n",
    "We are able to find known full conditional distributions for Z, $K_+$, and $\\alpha$, so we will update them using Gibbs Sampling. We will update $\\sigma_X$ and $\\sigma_A$ using random-walk Metropolis-Hastings.\n",
    "\n",
    "#### Prior Distributions\n",
    "\n",
    "We begin by setting a prior on on our parameter controlling $K_+$, $\\alpha$:\n",
    "\n",
    "$$\\alpha \\sim Gamma(1,1)$$\n",
    "\n",
    "We then set a prior on our latent feature binary matrix Z using the Indian Buffet Process prior:\n",
    "\n",
    "$$P(z_{ik} = 1 | \\textbf{z}_{-i,k}) = \\dfrac{n_{-i,k}}{N}$$\n",
    "\n",
    "Finally, we set a Poisson prior on the number of latent features $K_+$:\n",
    "\n",
    "$$K_+ \\sim Poisson(\\dfrac{\\alpha}{N})$$\n",
    "\n",
    "#### Full Conditional Distributions Used  for Gibbs Sampling\n",
    "\n",
    "Now that we have defined our likelihood and selected our priors, we will now define our full conditional distributions for Z, $K_+$, and $\\alpha$ to be used in Gibbs Sampling. Beginning with Z, we find the full conditional distribution to be:\n",
    "\n",
    "$$P(z_{ik}|X,Z_{-(i,k),},\\sigma_X,\\sigma_A) \\propto  P(X|Z,\\sigma_X, \\sigma_A) * P(z_{ik}=1|\\textbf{z}_{-i,k})$$\n",
    "\n",
    "To sample the number of new features $K_+$ for observation $i$, we use our data likelihood and our Poisson$(\\dfrac{\\alpha}{N})$ prior for $K_+$ and truncate this distribution for a range of values of $K_+$ up to 4 new features. We then use this to compute the probability distribution for $K_+$ and sample the number of new features from this distribution.\n",
    "\n",
    "#### Metropolis-Hasting Updates\n",
    "\n",
    "To update $\\sigma_X$ and $\\sigma_A$, we use random walk Metropolis-Hastings steps. For $\\sigma_X$, we generate a random value from a Uniform(-.05, .05) distribution and add this value to our current value of $\\sigma_X$ to get $\\sigma_X^*$. We then accept our new value of $\\sigma_X$ with probability:\n",
    "\n",
    "$$p = min(1, \\dfrac{P(X|Z, \\sigma_X^*, \\sigma_A}{P(X|Z, \\sigma_X, \\sigma_A})$$\n",
    "\n",
    "To update $\\sigma_A$, we follow the same proceedure as with $\\sigma_X$, replacing $\\sigma_X$ with $\\sigma_A$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiling & Optimization\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Used within the Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood Function\n",
    "\n",
    "The likelihood function is used to compute $P(X|Z,\\sigma_X, \\sigma_A)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Likelihood function used to compute P(X | Z, sigma_X, sigma_A)\n",
    "def likelihood(X, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim):\n",
    "    part1 = (-1)*num_objects*(0.5*object_dim)*np.log(2*np.pi)\n",
    "    part2 = (-1)*(num_objects-K_plus)* object_dim *np.log(sigma_X) \n",
    "    part3 = (-1)*object_dim*K_plus*np.log(sigma_A) \n",
    "    part4 = (-1)*(0.5*object_dim)* np.log(np.linalg.det((np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2)*np.eye(K_plus)))) \n",
    "    part5 = (-1/(2*sigma_X**2)) * np.trace(np.dot(np.dot(X.T,(np.identity(num_objects) - np.dot(np.dot(Z,M),Z.T))),X))\n",
    "    total = part1+part2+part3+part4+part5\n",
    "    return(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mcalc Function\n",
    "\n",
    "The Mcalc function is used to compute $(Z^T Z + (\\dfrac{\\sigma_X^2}{\\sigma_A^2}) * I)^{-1}$ which is used in several calculations throughout the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Mcalc(Z, sigma_X, sigma_A, K_plus):\n",
    "    M = np.linalg.inv(np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2) * np.eye(K_plus))\n",
    "    return(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Met_zval Function\n",
    "\n",
    "The Met_zval function is used to update $z_{i,k}$ using Gibbs Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function samples new value of Z[i,k] using Gibbs Sampling\n",
    "def Met_zval(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim, i, k):    \n",
    "    \n",
    "    P=np.zeros(2)\n",
    "\n",
    "    Z[i,k]=1\n",
    "    #Compute posterior density of new_sample\n",
    "    M = Mcalc(Z, sigma_X, sigma_A, K_plus)\n",
    "    P[0] = likelihood(data, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(np.sum(Z[:, k]) - Z[i,k]) - np.log(num_objects)\n",
    "\n",
    "    #Set new_sample to 0\n",
    "    Z[i,k]=0\n",
    "    #Computer posterior density of new_sample\n",
    "    M = Mcalc(Z, sigma_X, sigma_A, K_plus)\n",
    "    P[1] = likelihood(data, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(num_objects - np.sum(Z[:,k])) - np.log(num_objects)\n",
    "\n",
    "    P = np.exp(P - np.max(P))\n",
    "\n",
    "    if np.random.uniform(0,1) < (P[0]/(np.sum(P))):\n",
    "        new_sample = 1\n",
    "    else:\n",
    "        new_sample = 0\n",
    "    \n",
    "    return(new_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New_dishes Function\n",
    "\n",
    "The New_dishes function samples the number of new features for observation $i$ using Gibbs Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def New_dishes(data, Z, sigma_X, sigma_A, K_plus, alpha, num_objects, object_dim, trunc_val,i):\n",
    "    trunc = np.zeros(trunc_val)\n",
    "    alpha_N = alpha/num_objects\n",
    "\n",
    "    for k_i in range(0,trunc_val):\n",
    "        Z_temp = Z\n",
    "        if k_i>0:\n",
    "            newcol = np.zeros((num_objects, k_i))\n",
    "            newcol[i,:] = 1 \n",
    "            Z_temp = np.column_stack((Z_temp, newcol))\n",
    "        M = Mcalc(Z_temp, sigma_X, sigma_A, K_plus+k_i)\n",
    "        trunc[k_i] = k_i * np.log(alpha_N) - alpha_N - np.log(np.math.factorial(k_i)) + likelihood(data, Z_temp, M, sigma_A, sigma_X, K_plus+k_i, num_objects, object_dim)\n",
    "\n",
    "    trunc = np.exp(trunc - np.max(trunc))\n",
    "    trunc = trunc/np.sum(trunc)\n",
    "\n",
    "    p = np.random.uniform(0,1)\n",
    "    t = 0\n",
    "    new_dishes = 0\n",
    "\n",
    "    for k_i in range(0,trunc_val):\n",
    "        t = t + trunc[k_i]\n",
    "        if p < t:\n",
    "            new_dishes = k_i\n",
    "            break\n",
    "            \n",
    "    return(new_dishes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Met_sigma Function\n",
    "\n",
    "The Met_sigma function updates both $\\sigma_X$ and $\\sigma_A$ using random-walk Metropolis-Hastings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Met_sigma(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim):\n",
    "    \n",
    "    M = Mcalc(Z, sigma_X, sigma_A, K_plus)  \n",
    "    lik_curr = likelihood(data, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim)\n",
    "\n",
    "    if np.random.uniform(0,1) < 0.5:\n",
    "        sigma_X_new = sigma_X - np.random.uniform(0,1)/20\n",
    "    else:\n",
    "        sigma_X_new = sigma_X + np.random.uniform(0,1)/20\n",
    "\n",
    "    M = Mcalc(Z, sigma_X_new, sigma_A, K_plus)\n",
    "    lik_new_X = likelihood(data, Z, M, sigma_A, sigma_X_new, K_plus, num_objects, object_dim)\n",
    "\n",
    "    acc_X = np.exp(min(0, lik_new_X - lik_curr))\n",
    "\n",
    "    if np.random.uniform(0,1) < 0.5:\n",
    "        sigma_A_new = sigma_A - np.random.uniform(0,1)/20\n",
    "    else:\n",
    "        sigma_A_new = sigma_A + np.random.uniform(0,1)/20\n",
    "\n",
    "    M = Mcalc(Z, sigma_X, sigma_A_new, K_plus)\n",
    "    lik_new_A = likelihood(data, Z, M, sigma_A_new, sigma_X, K_plus, num_objects, object_dim)\n",
    "\n",
    "    acc_A = np.exp(min(0, lik_new_A - lik_curr))\n",
    "    \n",
    "    sigma_X_val=0\n",
    "    sigma_A_val=0\n",
    "\n",
    "    if np.random.uniform(0,1) < acc_X:\n",
    "        sigma_X_val = sigma_X_new\n",
    "    else:\n",
    "        sigma_X_val = sigma_X\n",
    "    \n",
    "    if np.random.uniform(0,1) < acc_A:\n",
    "        sigma_A_val = sigma_A_new\n",
    "    else:\n",
    "        sigma_A_val = sigma_A\n",
    "        \n",
    "    return list([sigma_X_val, sigma_A_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampler Function\n",
    "\n",
    "The Sampler function combines all of the above functions to run the entirety of our MCMC algorithm. It outputs the posterior distibutions for Z, $K_+$, $\\sigma_X$, $\\sigma_A$, and $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Sampler(data, num_objects, object_dim, E=1000,  K_inf = 20, sigma_X = 1, sigma_A = 1, alpha = 1, trunc_val=5):\n",
    "    #Set storage arrays for sampled parameters\n",
    "    chain_Z = np.zeros([E, num_objects, K_inf])\n",
    "    chain_K = np.zeros([E, 1])\n",
    "    chain_sigma_X = np.zeros([E, 1])\n",
    "    chain_sigma_A = np.zeros([E, 1])\n",
    "    chain_alpha = np.zeros([E, 1])\n",
    "\n",
    "    #Initialize parameter values\n",
    "    num_object= np.shape(data)[0]\n",
    "    object_dim = np.shape(data)[1]\n",
    "    [Z, K_plus] = sampleIBP(alpha, num_objects)\n",
    "\n",
    "    #Compute Harmonic Number\n",
    "    HN = 0\n",
    "    for i in range(0, num_objects):\n",
    "        HN = HN + 1.0/(i+1)\n",
    "\n",
    "    for e in range(0, E):\n",
    "        #Store sampled values\n",
    "        chain_Z[e, :, 0:K_plus] = Z[:, 0:K_plus]\n",
    "        chain_K[e] = K_plus\n",
    "        chain_sigma_X[e] = sigma_X\n",
    "        chain_sigma_A[e] = sigma_A\n",
    "        chain_alpha[e] = alpha\n",
    "\n",
    "        #if (e%100==0):\n",
    "        #    print(e)\n",
    "        #print(\"At iteration\", e, \": K_plus is\", K_plus, \", alpha is\", alpha) \n",
    "\n",
    "        #Generate a new value for Z[i,k] and accept by Metropolis\n",
    "        for i in range(0, num_objects):\n",
    "            #First we remove singular features if any\n",
    "            for k in range(0, K_plus):\n",
    "                if (k>=K_plus):\n",
    "                    break\n",
    "                if(Z[i, k] > 0):\n",
    "                    if (np.sum(Z[:, k]) - Z[i, k]) <= 0: \n",
    "                        Z[i, k] = 0\n",
    "                        Z[:, k:(K_plus - 1)] = Z[:, (k+1):K_plus]\n",
    "                        K_plus = K_plus - 1\n",
    "                        Z = Z[:, 0:K_plus]\n",
    "                        continue\n",
    "                #Compute conditional distribution for current cell\n",
    "                Z[i,k] = Met_zval(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim, i, k)\n",
    "\n",
    "            #Sample new dishes by Metropolis\n",
    "            new_dishes = New_dishes(data, Z, sigma_X, sigma_A, K_plus, alpha, num_objects, object_dim, trunc_val,i)\n",
    "\n",
    "            if(new_dishes > 0):\n",
    "                newcol = np.zeros((num_objects, new_dishes))\n",
    "                newcol[i,:] = 1\n",
    "                Z = np.column_stack((Z, newcol))\n",
    "            K_plus = K_plus + new_dishes\n",
    "\n",
    "        #Sample sigma_X and sigma_A through Metropolis\n",
    "        [sigma_X, sigma_A] = Met_sigma(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim)\n",
    "        #Sample alpha via Gibbs\n",
    "        alpha = np.random.gamma(1 + K_plus, 1/(1+HN))\n",
    "    \n",
    "    print(\"Complete\")\n",
    "    return list([chain_Z, chain_K, chain_sigma_X, chain_sigma_A, chain_alpha])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling Our Sampler\n",
    "\n",
    "In order to test the efficiency our code, we profile our Sampler function to find any potential bottlenecks in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n",
      "Fri Apr 29 17:21:49 2016    profiling_unoptimized\n",
      "\n",
      "         9773048 function calls in 66.686 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 65 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   66.686   66.686 {built-in method builtins.exec}\n",
      "        1    0.000    0.000   66.686   66.686 <string>:1(<module>)\n",
      "        1    0.290    0.290   66.686   66.686 <ipython-input-9-96d59dce4ebe>:1(Sampler)\n",
      "   133080    9.179    0.000   47.531    0.000 <ipython-input-4-cf0b3993be3a>:2(likelihood)\n",
      "    41390    1.970    0.000   41.285    0.001 <ipython-input-6-7997664e2c7c>:2(Met_zval)\n",
      "   798480   26.625    0.000   26.625    0.000 {built-in method numpy.core.multiarray.dot}\n",
      "    10000    1.440    0.000   24.778    0.002 <ipython-input-7-3b4495a7a593>:1(New_dishes)\n",
      "   133080    2.092    0.000   12.381    0.000 <ipython-input-5-c62396fa41c4>:1(Mcalc)\n",
      "   133080    4.295    0.000    7.242    0.000 linalg.py:458(inv)\n",
      "   133080    2.753    0.000    5.417    0.000 linalg.py:1723(det)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Profiling our code for 100 iterations of our Sampler Function\n",
    "import cProfile\n",
    "import re\n",
    "np.random.seed(1234)\n",
    "cProfile.run('Sampler(image_data, num_objects, object_dim, E=100,  K_inf = 20, sigma_X = 1, sigma_A = 1, alpha = 1, trunc_val=5)', \"profiling_unoptimized\")\n",
    "\n",
    "# Displaying results of our profiling\n",
    "import pstats\n",
    "p = pstats.Stats('profiling_unoptimized')\n",
    "p.strip_dirs().sort_stats(\"cumulative\").print_stats(10)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from our profiling output above that the biggest bottleneck within our code is our likelihood function. We will focus our optimization efforts on this function going forward. We notice that we call the Mcalc function along with likelihood function in tandem several times throughout our Sampler function. To reduce the number of calls and to speed up our Sampler function, we move the calculation of the M matrix into our likelihood function. This leads us to a new function we call likelihood_opt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood_opt Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Optimized likelihood\n",
    "# This function return the log likelihood\n",
    "def likelihood_opt(X, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim):\n",
    "    #Calculate M\n",
    "    M = np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2)*np.eye(K_plus)\n",
    "    \n",
    "    part1 = (-1)*num_objects*(0.5*object_dim)*np.log(2*np.pi)\n",
    "    part2 = (-1)*(num_objects-K_plus)* object_dim *np.log(sigma_X) \n",
    "    part3 = (-1)*object_dim*K_plus*np.log(sigma_A) \n",
    "    part4 = (-1)*(0.5*object_dim)* np.log(np.linalg.det(M)) \n",
    "    part5 = (-1/(2*sigma_X**2)) * np.trace(np.dot(np.dot(X.T,(np.identity(num_objects) - np.dot(np.dot(Z,np.linalg.inv(M)),Z.T))),X))\n",
    "    total = part1+part2+part3+part4+part5\n",
    "    return(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Likelihood Functions\n",
    "\n",
    "We now compare our optimized likelihood function likelihood_opt to our unoptimized version to see if we get any significant speed up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original Likelihood</th>\n",
       "      <td>0.00058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Opimized Likelihood</th>\n",
       "      <td>0.00041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Time\n",
       "Original Likelihood  0.00058\n",
       "Opimized Likelihood  0.00041"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Setting values to use in likelihood function comparisons\n",
    "num_objects = image_data.shape[0]\n",
    "object_dim = image_data.shape[1]\n",
    "\n",
    "sigma_X=1\n",
    "sigma_A=1\n",
    "alpha=1\n",
    "\n",
    "Z, K_plus = sampleIBP(alpha,num_objects)\n",
    "\n",
    "# Time the original likelihood function\n",
    "loops = 1000\n",
    "time_likelihood=np.zeros(loops)\n",
    "for l in range(loops):\n",
    "    t0=time.time()\n",
    "    M = Mcalc(Z, sigma_X, sigma_A, K_plus)\n",
    "    likelihood(image_data, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim)\n",
    "    t1=time.time()\n",
    "    time_likelihood[l]=t1-t0\n",
    "mean_time_likelihood= round(np.mean(time_likelihood),7)\n",
    "\n",
    "\n",
    "# Time the optimized likelihood function\n",
    "time_likelihood_opt = np.zeros(loops)\n",
    "for l in range(loops):\n",
    "    t0 = time.time()\n",
    "    likelihood_opt(image_data, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim)\n",
    "    t1 = time.time()\n",
    "    time_likelihood_opt[l] = t1-t0\n",
    "mean_time_likelihood_opt = round(np.mean(time_likelihood_opt), 7)\n",
    "\n",
    "time_array = np.array([mean_time_likelihood, mean_time_likelihood_opt])\n",
    "\n",
    "cols = [\"Time\"]\n",
    "index = [\"Original Likelihood\", \"Optimized Likelihood\"]\n",
    "\n",
    "time_df = pd.DataFrame(time_array, columns = cols, index = index)\n",
    "time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our optimized likelihood function produces a noticeable speed up over our original likelihood function. Over thousands of iterations, this small speed up should accumulate to produce a much faster sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampler_opt Function\n",
    "\n",
    "We rewrite our Sampler function with this new optimization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampler_opt(data, num_objects, object_dim, E=1000,  K_inf = 20, sigma_X = 1, sigma_A = 1, alpha = 1, trunc_val=5):\n",
    "    #Set storage arrays for sampled parameters\n",
    "    chain_Z = np.zeros([E, num_objects, K_inf])\n",
    "    chain_K = np.zeros([E, 1])\n",
    "    chain_sigma_X = np.zeros([E, 1])\n",
    "    chain_sigma_A = np.zeros([E, 1])\n",
    "    chain_alpha = np.zeros([E, 1])\n",
    "\n",
    "    #Initialize parameter values\n",
    "    num_object= np.shape(data)[0]\n",
    "    object_dim = np.shape(data)[1]\n",
    "    [Z, K_plus] = sampleIBP(alpha, num_objects)\n",
    "\n",
    "    #Compute Harmonic Number\n",
    "    HN = 0\n",
    "    for i in range(0, num_objects):\n",
    "        HN = HN + 1.0/(i+1)\n",
    "\n",
    "    for e in range(0, E):\n",
    "        #Store sampled values\n",
    "        chain_Z[e, :, 0:K_plus] = Z[:, 0:K_plus]\n",
    "        chain_K[e] = K_plus\n",
    "        chain_sigma_X[e] = sigma_X\n",
    "        chain_sigma_A[e] = sigma_A\n",
    "        chain_alpha[e] = alpha\n",
    "\n",
    "        if (e%100==0):\n",
    "            print(e)\n",
    "        print(\"At iteration\", e, \": K_plus is\", K_plus, \", alpha is\", alpha) \n",
    "\n",
    "        #Generate a new value for Z[i,k] and accept by Metropolis\n",
    "        for i in range(0, num_objects):\n",
    "            #First we remove singular features if any\n",
    "            for k in range(0, K_plus):\n",
    "                if (k>=K_plus):\n",
    "                    break\n",
    "                if(Z[i, k] > 0):\n",
    "                    if (np.sum(Z[:, k]) - Z[i, k]) <= 0: \n",
    "                        Z[i, k] = 0\n",
    "                        Z[:, k:(K_plus - 1)] = Z[:, (k+1):K_plus]\n",
    "                        K_plus = K_plus - 1\n",
    "                        Z = Z[:, 0:K_plus]\n",
    "                        continue\n",
    "                #Compute conditional distribution for current cell\n",
    "                Z[i,k] = Met_zval(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim, i, k)\n",
    "\n",
    "\n",
    "            #Sample new dishes by Metropolis\n",
    "            new_dishes = New_dishes(data, Z, sigma_X, sigma_A, K_plus, alpha, num_objects, object_dim, trunc_val,i)\n",
    "\n",
    "            if(new_dishes > 0):\n",
    "                newcol = np.zeros((num_objects, new_dishes))\n",
    "                newcol[i,:] = 1\n",
    "                Z = np.column_stack((Z, newcol))\n",
    "            K_plus = K_plus + new_dishes\n",
    "\n",
    "        #Sample sigma_X and sigma_A through Metropolis\n",
    "        [sigma_X, sigma_A] = Met_sigma(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim)\n",
    "        #Sample alpha via Gibbs\n",
    "        alpha = np.random.gamma(1 + K_plus, 1/(1+HN))\n",
    "    \n",
    "    print(\"Complete\")\n",
    "    return list([chain_Z, chain_K, chain_sigma_X, chain_sigma_A, chain_alpha])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
