{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indian Buffet Process Prior\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indian Buffet Process Function\n",
    "\n",
    "def sampleIBP(alpha, num_objects):\n",
    "    \n",
    "    # Initializing storage for results\n",
    "    result = np.zeros([num_objects, 1000])\n",
    "    # Draw from the prior for alpha\n",
    "    t = np.random.poisson(alpha)\n",
    "    # Filling in first row of result matrix\n",
    "    result[0, 0:t] = np.ones([1, t])\n",
    "    # Initializing K+\n",
    "    K_plus = t\n",
    "    \n",
    "\n",
    "    for i in range(1, num_objects):\n",
    "        for j in range(0, K_plus):\n",
    "            p = np.array([np.log(np.sum(result[0:i,j])) - np.log(i+1), \n",
    "                          np.log(i+1 - np.sum(result[0:i, j])) - np.log(i+1)])\n",
    "            p = np.exp(p - max(p))\n",
    "\n",
    "            if(np.random.uniform() < p[0]/np.sum(p)):\n",
    "                result[i, j] = 1\n",
    "            else:\n",
    "                result[i, j] = 0\n",
    "        t = np.random.poisson(alpha/(i+1))\n",
    "        x = K_plus + 1\n",
    "        y = K_plus + t\n",
    "        result[i, (x-1):y] = np.ones([1, t]) # NEED TO CHECK DIM\n",
    "        K_plus = K_plus+t\n",
    "    result = result[:, 0:K_plus]\n",
    "    return list([result, K_plus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.,  1.]]), 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleIBP(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse Simplification\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for enhancing inverse process\n",
    "\n",
    "def calcInverse(Z, M, i, k, val):\n",
    "    M_i = M - (np.dot(np.dot(np.dot(M, Z[i,:].T), Z[i,:]), M)) / (np.dot(np.dot(Z[i,:], M), Z[i,:].T) - 1)\n",
    "    Z[i, k] = val\n",
    "    M = M_i - (np.dot(np.dot(np.dot(M_i, Z[i,:].T), Z[i,:]), M_i)) / (np.dot(np.dot(Z[i,:], M_i), Z[i,:].T) + 1)\n",
    "    Inv = M\n",
    "    return(Inv)\n",
    "\n",
    "def calcInverse1(Z, M, i, k, val):\n",
    "    # Calculating M_i\n",
    "    part1 = M\n",
    "    part2 = np.dot(M, Z[i,:].T)\n",
    "    part3 = np.dot(part2, Z[i,:])\n",
    "    part4 = np.dot(part3, M)\n",
    "    part5 = np.dot(Z[i,:], M)\n",
    "    part6 = np.dot(part5, Z[i,:].T) - 1\n",
    "\n",
    "    M_i = part1 - part4 / part6\n",
    "\n",
    "    Z[i, k] = val\n",
    "\n",
    "    #Calculating M\n",
    "    part1 = M_i\n",
    "    part2 = np.dot(M_i, Z[i,:].T)\n",
    "    part3 = np.dot(part2, Z[i,:])\n",
    "    part4 = np.dot(part3, M_i)\n",
    "    part5 = np.dot(Z[i,:], M_i)\n",
    "    part6 = np.dot(part5, Z[i,:].T) + 1\n",
    "\n",
    "    M = part1 - part4 / part6\n",
    "\n",
    "    Inv = M\n",
    "    return(Inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood Function \n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define the likelihood function ##\n",
    "# The function returns the log likelihood\n",
    "def likelihood(X, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim):\n",
    "    part1 = - (num_objects*(object_dim/2)*np.log(2*np.pi))\n",
    "    part2 = - (num_objects-K_plus)* object_dim *np.log(sigma_X) \n",
    "    part3 = - (object_dim*K_plus)*np.log(sigma_A) \n",
    "    part4 = - (object_dim/2)* np.log(np.dot(np.transpose(Z),Z) + (sigma_X/sigma_A)**2 * np.identity(K_plus))\n",
    "    part5 = (-1/(2*sigma_X**2)) * np.trace(np.dot(np.dot(np.transpose(X),(np.identity(num_objects) - np.dot(np.dot(Z,M),np.transpose(Z)))),X))\n",
    "    total = part1+part2+part3+part4+part5\n",
    "    return(total)\n",
    "\n",
    "def likelihood1(X, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim):\n",
    "    log_ll = -1 * num_objects * object_dim * .5 * np.log(2*np.pi) - 1 * (num_objects - K_plus) * object_dim * np.log(sigma_X) - K_plus * object_dim * np.log(sigma_A) - object_dim * .5 * np.log(np.linalg.det((np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2)*np.eye(K_plus)))) + (-1/(2*sigma_X**2)) * np.trace(np.dot(np.dot(X.T, np.eye(num_objects) - np.dot(Z, np.dot(M, Z.T))), X))\n",
    "    return(log_ll)\n",
    "\n",
    "def likelihood2(X, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim):\n",
    "    M = np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2) * np.eye(K_plus)\n",
    "    log_ll = -1 * num_objects * object_dim * .5 * np.log(2*np.pi) - 1 * (num_objects - K_plus) * object_dim * np.log(sigma_X) - K_plus * object_dim * np.log(sigma_A) - object_dim * .5 * np.log(np.linalg.det(M)) + (-1/(2*sigma_X**2)) * np.trace(np.dot(np.dot(X.T, np.eye(num_objects) - np.dot(Z, np.dot(np.linalg.inv(M), Z.T))), X))\n",
    "    return(log_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ll(X, Z, sigmaX, sigmaA, K, D, N):\n",
    "    \"\"\"Improved log likelihood calculation for X\"\"\"\n",
    "    import numpy as np\n",
    "    import math\n",
    "    M = Z.T.dot(Z)+(sigmaX**2/sigmaA**2)*np.identity(K)\n",
    "    return (-1)*np.log(2*np.pi)*N*D*.5 - np.log(sigmaX)*(N-K)*D - np.log(sigmaA)*K*D - .5*D*np.log(np.linalg.det(M)) \\\n",
    "        -.5/(sigmaX**2)*np.trace( (X.T.dot( np.identity(N)-Z.dot(np.linalg.inv(M).dot(Z.T)) )).dot(X) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metropolis Functions\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import uniform as unif\n",
    "\n",
    "def met_sigma_A(Z, K_plus, new_dishes, sigma_X, sigma_A, num_objects, object_dim, X):\n",
    "    \n",
    "    #Determine M based on sigma_A\n",
    "    #Determine current likelihood based on M and sigma_A\n",
    "    M = Mcalc(Z, K_plus, new_dishes, sigma_X, sigma_A)\n",
    "    l_curr = likelihood1(X, Z[:,0:(K_plus+new_dishes)], M, sigma_A, sigma_X, K_plus+new_dishes, num_objects, object_dim)\n",
    "    \n",
    "    #sample new value from symmetric distribution close to current value\n",
    "    if unif.rvs(0,1)<0.5:\n",
    "        new_sigma_A = sigma_A - unif.rvs(0,1)/20\n",
    "    else:\n",
    "        new_sigma_A = sigma_A + unif.rvs(0,1)/20\n",
    "    \n",
    "    #Determine M based on new_sigma_A\n",
    "    #Determine current likelihood based on M and new_sigma_A\n",
    "    M = Mcalc(Z, K_plus, new_dishes, sigma_X, new_sigma_A)\n",
    "    l_new = likelihood1(x, Z[:,0:(K_plus+new_dishes)], M, new_sigma_A, sigma_X, K_plus+new_dishes, num_objects, object_dim)\n",
    "    \n",
    "    acceptance_ratio = np.exp(min(0,l_new - l_curr))\n",
    "    \n",
    "    if unif.rvs(0,1)<acceptance_ratio:\n",
    "        return(new_sigma_A)\n",
    "    else:\n",
    "        return(sigma_A)\n",
    "\n",
    "def met_sigma_X(Z, K_plus, new_dishes, sigma_X, sigma_A, num_objects, object_dim, X):\n",
    "    \n",
    "    #Determine M based on sigma_X\n",
    "    #Determine current likelihood based on M and sigma_X\n",
    "    M = Mcalc(Z, K_plus, new_dishes, sigma_X, sigma_A)\n",
    "    l_curr = likelihood1(X, Z[:,0:(K_plus+new_dishes)], M, sigma_A, sigma_X, K_plus+new_dishes, num_objects, object_dim)\n",
    "    \n",
    "    #sample new value from symmetric distribution close to current value\n",
    "    if unif.rvs(0,1)<0.5:\n",
    "        new_sigma_X = sigma_X - unif.rvs(0,1)/20\n",
    "    else:\n",
    "        new_sigma_X = sigma_X + unif.rvs(0,1)/20\n",
    "    \n",
    "    #Determine M based on new_sigma_A\n",
    "    #Determine current likelihood based on M and new_sigma_A\n",
    "    M = Mcalc(Z, K_plus, new_dishes, new_sigma_X, sigma_A)\n",
    "    l_new = likelihood1(X, Z[:,0:(K_plus+new_dishes)], M, sigma_A, new_sigma_X, K_plus+new_dishes, num_objects, object_dim)\n",
    "    \n",
    "    acceptance_ratio = np.exp(min(0,l_new - l_curr))\n",
    "    \n",
    "    if unif.rvs(0,1)<acceptance_ratio:\n",
    "        return(new_sigma_X)\n",
    "    else:\n",
    "        return(sigma_X)\n",
    "\n",
    "def met_sigma_A1(Z, K_plus, new_dishes, sigma_X, sigma_A, num_objects, object_dim, X):\n",
    "    \n",
    "    #Determine M based on sigma_A\n",
    "    #Determine current likelihood based on M and sigma_A\n",
    "    #M = Mcalc(Z, K_plus, new_dishes, sigma_X, sigma_A)\n",
    "    l_curr = likelihood2(X, Z[:,0:(K_plus+new_dishes)], sigma_A, sigma_X, K_plus+new_dishes, num_objects, object_dim)\n",
    "    \n",
    "    #sample new value from symmetric distribution close to current value\n",
    "    if unif.rvs(0,1)<0.5:\n",
    "        new_sigma_A = sigma_A - unif.rvs(0,1)/20\n",
    "    else:\n",
    "        new_sigma_A = sigma_A + unif.rvs(0,1)/20\n",
    "    \n",
    "    #Determine M based on new_sigma_A\n",
    "    #Determine current likelihood based on M and new_sigma_A\n",
    "    #M = Mcalc(Z, K_plus, new_dishes, sigma_X, new_sigma_A)\n",
    "    l_new = likelihood2(x, Z[:,0:(K_plus+new_dishes)], new_sigma_A, sigma_X, K_plus+new_dishes, num_objects, object_dim)\n",
    "    \n",
    "    acceptance_ratio = np.exp(min(0,l_new - l_curr))\n",
    "    \n",
    "    if unif.rvs(0,1)<acceptance_ratio:\n",
    "        return(new_sigma_A)\n",
    "    else:\n",
    "        return(sigma_A)\n",
    "\n",
    "def met_sigma_X1(Z, K_plus, new_dishes, sigma_X, sigma_A, num_objects, object_dim, X):\n",
    "    \n",
    "    #Determine M based on sigma_X\n",
    "    #Determine current likelihood based on M and sigma_X\n",
    "    #M = Mcalc(Z, K_plus, new_dishes, sigma_X, sigma_A)\n",
    "    l_curr = likelihood2(X, Z[:,0:(K_plus+new_dishes)], sigma_A, sigma_X, K_plus+new_dishes, num_objects, object_dim)\n",
    "    \n",
    "    #sample new value from symmetric distribution close to current value\n",
    "    if unif.rvs(0,1)<0.5:\n",
    "        new_sigma_X = sigma_X - unif.rvs(0,1)/20\n",
    "    else:\n",
    "        new_sigma_X = sigma_X + unif.rvs(0,1)/20\n",
    "    \n",
    "    #Determine M based on new_sigma_A\n",
    "    #Determine current likelihood based on M and new_sigma_A\n",
    "    #M = Mcalc(Z, K_plus, new_dishes, new_sigma_X, sigma_A)\n",
    "    l_new = likelihood2(X, Z[:,0:(K_plus+new_dishes)], sigma_A, new_sigma_X, K_plus+new_dishes, num_objects, object_dim)\n",
    "    \n",
    "    acceptance_ratio = np.exp(min(0,l_new - l_curr))\n",
    "    \n",
    "    if unif.rvs(0,1)<acceptance_ratio:\n",
    "        return(new_sigma_X)\n",
    "    else:\n",
    "        return(sigma_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mcalc Function\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Mcalc(Z, K_plus, new_dishes, sigma_X, sigma_A):\n",
    "    part1 = np.dot(np.transpose(Z[:,0:(K_plus+new_dishes)]), Z[:,0:(K_plus+new_dishes)])\n",
    "    part2 = (sigma_X/sigma_A)**2 * np.identity(K_plus+new_dishes)\n",
    "    result = np.linalg.inv(part1+part2)\n",
    "    return(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Generation\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal as mvtnorm\n",
    "from scipy.stats import bernoulli\n",
    "import numpy as np\n",
    "\n",
    "### Data Simulation ###\n",
    "\n",
    "## Features/Latent Variables #\n",
    "#Each row of the weight matrix \"W\" specifies one base images(feature/latent variables)\"\n",
    "\n",
    "W = np.array([[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]])\n",
    "\n",
    "\n",
    "#Each image in our simulated data set is the superposition of four base images#\n",
    "# Number of images/ data points\n",
    "num_objects=100\n",
    "\n",
    "#Dimension of image (6x6)\n",
    "object_dim  = 6*6\n",
    "\n",
    "#Covariance matrix for images/ white noise\n",
    "sigma_x_orig = 0.5\n",
    "I = sigma_x_orig * np.identity(object_dim)\n",
    "\n",
    "#z_i - binary feature matrix (1 x 4) - each entry set to 1 with probability 0.5 and 0 otherwise#\n",
    "#x is data variable - each row correspondes to a superimposed built from a random combination of latent features#\n",
    "#with white noise added - x is built with multivariate gaussian#\n",
    "x = np.zeros((100,36))\n",
    "z = np.zeros((100,4))\n",
    "\n",
    "for i in range(0,num_objects):\n",
    "    z[i,:] = np.array([bernoulli.rvs(p=0.5, size=4)])\n",
    "    x[i,:] = mvtnorm.rvs(np.dot(z[i,:],W), I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sampler\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0 : K_plus is 5 , alpha is 1\n",
      "At iteration 1 : K_plus is 4 , alpha is 1.3963298378211284\n",
      "At iteration 2 : K_plus is 2 , alpha is 0.6169403505179917\n",
      "At iteration 3 : K_plus is 2 , alpha is 0.23767865160230417\n",
      "At iteration 4 : K_plus is 1 , alpha is 0.7427434824493387\n",
      "At iteration 5 : K_plus is 1 , alpha is 0.5295302638421566\n",
      "At iteration 6 : K_plus is 1 , alpha is 1.0437120795599208\n",
      "At iteration 7 : K_plus is 2 , alpha is 0.6731305353864493\n",
      "At iteration 8 : K_plus is 2 , alpha is 0.8003158668776872\n",
      "At iteration 9 : K_plus is 3 , alpha is 0.5608862853738201\n",
      "At iteration 10 : K_plus is 3 , alpha is 0.3115594150053396\n",
      "At iteration 11 : K_plus is 3 , alpha is 0.7312325926511918\n",
      "At iteration 12 : K_plus is 3 , alpha is 0.24199112464864006\n",
      "At iteration 13 : K_plus is 2 , alpha is 0.23761165011956945\n",
      "At iteration 14 : K_plus is 3 , alpha is 0.8536931636391178\n",
      "At iteration 15 : K_plus is 3 , alpha is 0.2550275984592406\n",
      "At iteration 16 : K_plus is 2 , alpha is 0.28564608099693\n",
      "At iteration 17 : K_plus is 2 , alpha is 0.4621087833483477\n",
      "At iteration 18 : K_plus is 3 , alpha is 0.7685343602376444\n",
      "At iteration 19 : K_plus is 3 , alpha is 0.8116587788366864\n",
      "At iteration 20 : K_plus is 3 , alpha is 1.1215241066071864\n",
      "At iteration 21 : K_plus is 3 , alpha is 0.7617852108949622\n",
      "At iteration 22 : K_plus is 3 , alpha is 0.6247489547081152\n",
      "At iteration 23 : K_plus is 3 , alpha is 0.39803029184600514\n",
      "At iteration 24 : K_plus is 3 , alpha is 0.8485827044474299\n",
      "At iteration 25 : K_plus is 3 , alpha is 0.6451416192797496\n",
      "At iteration 26 : K_plus is 3 , alpha is 0.7035043360094859\n",
      "At iteration 27 : K_plus is 4 , alpha is 0.6344340401899707\n",
      "At iteration 28 : K_plus is 3 , alpha is 0.6946865317828578\n",
      "At iteration 29 : K_plus is 3 , alpha is 0.528850446516153\n",
      "At iteration 30 : K_plus is 3 , alpha is 1.0128787010923297\n",
      "At iteration 31 : K_plus is 3 , alpha is 0.3048364432568144\n",
      "At iteration 32 : K_plus is 3 , alpha is 0.3941335449747488\n",
      "At iteration 33 : K_plus is 3 , alpha is 0.7355404549000544\n",
      "At iteration 34 : K_plus is 3 , alpha is 1.0393854761965313\n",
      "At iteration 35 : K_plus is 3 , alpha is 0.2780056181062957\n",
      "At iteration 36 : K_plus is 3 , alpha is 0.24958331388421232\n",
      "At iteration 37 : K_plus is 3 , alpha is 0.7993662702854073\n",
      "At iteration 38 : K_plus is 3 , alpha is 0.3561762316490411\n",
      "At iteration 39 : K_plus is 3 , alpha is 0.9578691764880446\n",
      "At iteration 40 : K_plus is 5 , alpha is 0.9069956177201225\n",
      "At iteration 41 : K_plus is 4 , alpha is 0.6032898083247411\n",
      "At iteration 42 : K_plus is 2 , alpha is 0.3008643170858651\n",
      "At iteration 43 : K_plus is 2 , alpha is 0.7449465142187107\n",
      "At iteration 44 : K_plus is 4 , alpha is 1.0048236595045248\n",
      "At iteration 45 : K_plus is 5 , alpha is 0.7319364663117087\n",
      "At iteration 46 : K_plus is 4 , alpha is 0.45359294715087184\n",
      "At iteration 47 : K_plus is 4 , alpha is 1.1223270758142414\n",
      "At iteration 48 : K_plus is 5 , alpha is 0.5766206309767831\n",
      "At iteration 49 : K_plus is 3 , alpha is 0.3248159157819937\n",
      "At iteration 50 : K_plus is 5 , alpha is 1.0171462506646176\n",
      "At iteration 51 : K_plus is 4 , alpha is 0.8987870604569285\n",
      "At iteration 52 : K_plus is 4 , alpha is 0.5414356727597897\n",
      "At iteration 53 : K_plus is 4 , alpha is 0.8282158173036333\n",
      "At iteration 54 : K_plus is 3 , alpha is 0.8629698764745588\n",
      "At iteration 55 : K_plus is 4 , alpha is 0.4470504796353772\n",
      "At iteration 56 : K_plus is 4 , alpha is 1.1071414240044835\n",
      "At iteration 57 : K_plus is 5 , alpha is 1.246460770143652\n",
      "At iteration 58 : K_plus is 8 , alpha is 2.258761012353203\n",
      "At iteration 59 : K_plus is 7 , alpha is 1.3286244523576531\n",
      "At iteration 60 : K_plus is 7 , alpha is 1.385053682478776\n",
      "At iteration 61 : K_plus is 7 , alpha is 0.6151847771511083\n",
      "At iteration 62 : K_plus is 6 , alpha is 2.1849291207887447\n",
      "At iteration 63 : K_plus is 6 , alpha is 1.3706532082368934\n",
      "At iteration 64 : K_plus is 5 , alpha is 1.1076175031488968\n",
      "At iteration"
     ]
    }
   ],
   "source": [
    "# GENERAL FUNCTION SAMPLER\n",
    "\n",
    "# Harmonic number or N\n",
    "HN = 0\n",
    "for i in range(0, num_objects):\n",
    "    HN = HN + 1/(i+1)\n",
    "\n",
    "E = 1000\n",
    "BURN_IN = 0\n",
    "SAMPLE_SIZE = 1000\n",
    "\n",
    "# Initializing values for use in our chain\n",
    "sigma_A = 1\n",
    "sigma_X = 1\n",
    "# Poisson rate\n",
    "alpha = 1\n",
    "# Prespecified maximum number of latent features\n",
    "K_inf = 20\n",
    "# Indian Buffet Process Prior\n",
    "Z, K_plus = sampleIBP(alpha, num_objects)\n",
    "# Initialization of our chain for Z\n",
    "chain_Z = np.zeros([SAMPLE_SIZE, num_objects, K_inf])\n",
    "# Initialization of our chain for K\n",
    "chain_K = np.zeros([SAMPLE_SIZE, 1])\n",
    "# Initialization of our chain for sigma_X\n",
    "chain_sigma_X = np.zeros([SAMPLE_SIZE, 1])\n",
    "# Initialization of our chain for sigma_A\n",
    "chain_sigma_A = np.zeros([SAMPLE_SIZE, 1])\n",
    "# Initialization of our chain for alpha\n",
    "chain_alpha = np.zeros([SAMPLE_SIZE, 1])\n",
    "\n",
    "# Initializing storage for post burn-in samples\n",
    "s_counter = 0\n",
    "for e in range(0, E):\n",
    "    if(e > BURN_IN):\n",
    "        s_counter = s_counter + 1\n",
    "        chain_Z[s_counter, :, 0:K_plus] = Z[:, 0:K_plus]\n",
    "        chain_K[s_counter] = K_plus\n",
    "        chain_sigma_X[s_counter] = sigma_X\n",
    "        chain_sigma_A[s_counter] = sigma_A\n",
    "        chain_alpha[s_counter] = alpha\n",
    "    print(\"At iteration\", e, \": K_plus is\", K_plus, \", alpha is\", alpha) \n",
    "\n",
    "    for i in range(0, num_objects):\n",
    "        # M matrix will be handy for future computations\n",
    "        # SOMETIMES SINGULAR, NEED TO FIX\n",
    "        #M = np.linalg.inv(np.dot(Z[:, 0:K_plus].T, Z[:, 0:K_plus]) + (sigma_X**2/sigma_A**2) * np.eye(K_plus))\n",
    "        for k in range(0, K_plus):\n",
    "            # Checking to make sure that k < K_plus\n",
    "            if k >= K_plus:\n",
    "                break\n",
    "            if Z[i, k] > 0:\n",
    "                # Take care of singularities\n",
    "                if (np.sum(Z[:, k]) - Z[i, k]) <= 0: \n",
    "                    Z[i, k] = 0\n",
    "                    Z[:, k:(K_plus - 1)] = Z[:, (k+1):K_plus]\n",
    "                    K_plus = K_plus - 1\n",
    "                    Z = Z[:, 0:K_plus]\n",
    "                    #M = np.linalg.inv(np.dot(Z[:, 0:K_plus].T, Z[:, 0:K_plus]) + (sigma_X**2/sigma_A**2) * np.eye(K_plus))\n",
    "                    continue\n",
    "             \n",
    "            # This is where he has his calcInverse functions to \n",
    "            # speed up inverse calculations. I just use inverse\n",
    "            # of Z matrix below\n",
    "            #M1 = calcInverse(Z[:, 0:K_plus], M, i, k, 1)\n",
    "            #M2 = calcInverse(Z[:, 0:K_plus], M, i, k, 0)\n",
    "\n",
    "            # Compute conditional distributions for the current cell in Z\n",
    "            P = np.zeros(2)\n",
    "            Z[i, k] = 1\n",
    "            #P[0] = likelihood1(x, Z[:, 0:K_plus], M1, sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(np.sum(Z[:, k]) - 1) - np.log(num_objects)\n",
    "            P[0] = likelihood2(x, Z[:, 0:K_plus], sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(np.sum(Z[:, k]) - 1) - np.log(num_objects)\n",
    "\n",
    "            Z[i, k] = 0\n",
    "            #P[1] = likelihood1(x, Z[:, 0:K_plus], M2, sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(num_objects - np.sum(Z[:,k])) - np.log(num_objects)\n",
    "            P[1] = likelihood2(x, Z[:, 0:K_plus], sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(num_objects - np.sum(Z[:,k])) - np.log(num_objects)\n",
    "            \n",
    "            P = np.exp(P - np.max(P))\n",
    "\n",
    "            # Sample from the conditional\n",
    "            if np.random.uniform() < (P[0] / (np.sum(P))):\n",
    "                Z[i, k] = 1\n",
    "                #M = M1\n",
    "            else:\n",
    "                Z[i, k] = 0\n",
    "                #M = M2\n",
    "        \n",
    "        # Sample the number of new dishes for the current object\n",
    "        trun = np.zeros([1, 5])\n",
    "        alpha_N = alpha / num_objects\n",
    "\n",
    "\n",
    "################### ERROR IN DIMENSIONALITY RIGHT HERE ###########################        \n",
    "        for k_i in range(5):\n",
    "            Z_temp = Z        \n",
    "            if k_i > 0:\n",
    "                z = np.zeros((num_objects, k_i))\n",
    "                Z_temp = np.column_stack((Z_temp, z))\n",
    "                Z_temp[i, K_plus:(K_plus+k_i)] = 1\n",
    "            #M = np.linalg.inv(np.dot(Z_temp[:, 0:(K_plus + k_i)].T, Z_temp[:, 0:(K_plus+k_i)]) + (sigma_X**2/sigma_A**2) * np.eye(K_plus + k_i)) # MINUS ONE TO FIX DIMENSIONALITY MAY BE INCORRECT\n",
    "            #trun[0, k_i] = k_i * np.log(alpha_N) - alpha_N - np.log(np.math.factorial(k_i)) + likelihood1(x, Z_temp[:, 0:(K_plus + k_i)], M, sigma_A, sigma_X, K_plus + k_i, num_objects, object_dim)\n",
    "            trun[0, k_i] = k_i * np.log(alpha_N) - alpha_N - np.log(np.math.factorial(k_i)) + likelihood2(x, Z_temp[:, 0:(K_plus + k_i)], sigma_A, sigma_X, K_plus + k_i, num_objects, object_dim)\n",
    "        #Z[i, K_plus:(K_plus+4)] = 0\n",
    "        trun = np.exp(trun - np.max(trun))\n",
    "        trun = trun/np.sum(trun)\n",
    "        p = np.random.uniform()\n",
    "        t = 0\n",
    "        new_dishes = 0\n",
    "        for k_i in range(0,5):\n",
    "            t = t + trun[0, k_i]\n",
    "            if p < t:\n",
    "                new_dishes = k_i\n",
    "                break\n",
    "        if(new_dishes > 0):\n",
    "            z = np.zeros((num_objects, new_dishes))\n",
    "            Z = np.column_stack([Z, z])\n",
    "            Z[i, K_plus:(K_plus + new_dishes)] = 1\n",
    "        K_plus = K_plus + new_dishes\n",
    "    \n",
    "    # Metropolis steps for sampling sigma_X and sigma_A\n",
    "    likelihood_current = likelihood2(x, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim)\n",
    "    \n",
    "    if np.random.uniform(0,1) < .5:\n",
    "        sigma_X_new = sigma_X - np.random.uniform(0,1)/20\n",
    "    else:\n",
    "        sigma_X_new = sigma_X + np.random.uniform(0,1)/20\n",
    "\n",
    "    likelihood_new = likelihood2(x, Z, sigma_A, sigma_X_new, K_plus, num_objects, object_dim)\n",
    "    acc_X = np.exp(min(0, likelihood_new - likelihood_current))\n",
    "\n",
    "    if np.random.uniform(0,1) < .5:\n",
    "        sigma_A_new = sigma_A - np.random.uniform(0,1)/20\n",
    "    else:\n",
    "        sigma_A_new = sigma_A + np.random.uniform(0,1)/20\n",
    "    \n",
    "    likelihood_new = likelihood2(x, Z, sigma_A_new, sigma_X, K_plus, num_objects, object_dim)\n",
    "    acc_A = np.exp(min(0, likelihood_new - likelihood_current))\n",
    "\n",
    "    if np.random.uniform(0,1) < acc_X:\n",
    "        sigma_X = sigma_X_new\n",
    "    if np.random.uniform(0,1) < acc_A:\n",
    "        sigma_A = sigma_A_new\n",
    "   \n",
    "\n",
    "    #N_sigma_X = met_sigma_X1(Z, K_plus, new_dishes, sigma_X, sigma_A, num_objects, object_dim, x)\n",
    "\n",
    "    #sigma_A = met_sigma_A1(Z, K_plus, new_dishes, sigma_X, sigma_A, num_objects, object_dim, x)\n",
    "    \n",
    "    #sigma_X = N_sigma_X\n",
    "\n",
    "    # Sample alpha from its conditional posterior\n",
    "    alpha = np.random.gamma(1 + K_plus, 1/(1+HN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = Z[:,0:(K_plus+new_dishes)]\n",
    "K_plus = K_plus+new_dishes\n",
    "K_plus\n",
    "#np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2) * np.eye(K_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=float64), 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for k in range(10):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def profiler(n):\n",
    "    # GENERAL FUNCTION SAMPLER\n",
    "\n",
    "    # Harmonic number or N\n",
    "    HN = 0\n",
    "    for i in range(0, num_objects):\n",
    "        HN = HN + 1/(i+1)\n",
    "\n",
    "    E = n\n",
    "    BURN_IN = 0\n",
    "    SAMPLE_SIZE = n\n",
    "\n",
    "    # Initializing values for use in our chain\n",
    "    sigma_A = 1\n",
    "    sigma_X = 1\n",
    "    # Poisson rate\n",
    "    alpha = 1\n",
    "    # Prespecified maximum number of latent features\n",
    "    K_inf = 10\n",
    "    # Indian Buffet Process Prior\n",
    "    Z, K_plus = sampleIBP(alpha, num_objects)\n",
    "    # Initialization of our chain for Z\n",
    "    chain_Z = np.zeros([SAMPLE_SIZE, num_objects, K_inf])\n",
    "    # Initialization of our chain for K\n",
    "    chain_K = np.zeros([SAMPLE_SIZE, 1])\n",
    "    # Initialization of our chain for sigma_X\n",
    "    chain_sigma_X = np.zeros([SAMPLE_SIZE, 1])\n",
    "    # Initialization of our chain for sigma_A\n",
    "    chain_sigma_A = np.zeros([SAMPLE_SIZE, 1])\n",
    "    # Initialization of our chain for alpha\n",
    "    chain_alpha = np.zeros([SAMPLE_SIZE, 1])\n",
    "\n",
    "    # Initializing storage for post burn-in samples\n",
    "    s_counter = 0\n",
    "    for e in range(0, E):\n",
    "        if(e > BURN_IN):\n",
    "            s_counter = s_counter + 1\n",
    "            chain_Z[s_counter, :, 0:K_plus] = Z[:, 0:K_plus]\n",
    "            chain_K[s_counter] = K_plus\n",
    "            chain_sigma_X[s_counter] = sigma_X\n",
    "            chain_sigma_A[s_counter] = sigma_A\n",
    "            chain_alpha[s_counter] = alpha\n",
    "        print(\"At iteration\", e, \": K_plus is\", K_plus, \", alpha is\", alpha) \n",
    "\n",
    "        for i in range(0, num_objects):\n",
    "            # M matrix will be handy for future computations\n",
    "            # SOMETIMES SINGULAR, NEED TO FIX\n",
    "            M = np.linalg.inv(np.dot(Z[:, 0:K_plus].T, Z[:, 0:K_plus]) + (sigma_X**2/sigma_A**2) * np.eye(K_plus))\n",
    "            for k in range(0, K_plus):\n",
    "                # Checking to make sure that k < K_plus\n",
    "                if k+1 > K_plus:\n",
    "                    break\n",
    "                if Z[i, k] > 0:\n",
    "                    # Take care of singularities\n",
    "                    if np.sum(Z[:, k]) - Z[i, k] <= 0:\n",
    "                        Z[i, k] = 0\n",
    "                        Z[:, k:(K_plus - 1)] = Z[:, (k+1):K_plus]\n",
    "                        K_plus = K_plus - 1\n",
    "                        M = np.linalg.inv(np.dot(Z[:, 0:K_plus].T, Z[:, 0:K_plus]) + (sigma_X**2/sigma_A**2) * np.eye(K_plus))\n",
    "                        continue\n",
    "             \n",
    "                # This is where he has his calcInverse functions to \n",
    "                # speed up inverse calculations. I just use inverse\n",
    "                # of Z matrix below\n",
    "                M1 = calcInverse(Z[:, 0:K_plus], M, i, k, 1)\n",
    "                M2 = calcInverse(Z[:, 0:K_plus], M, i, k, 0)\n",
    "\n",
    "                # Compute conditional distributions for the current cell in Z\n",
    "                P = []\n",
    "                Z[i, k] = 1\n",
    "                P.append(likelihood1(x, Z[:, 0:K_plus], M1, sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(sum(Z[:, k]) - Z[i, k]) - np.log(num_objects))\n",
    "\n",
    "                Z[i, k] = 0\n",
    "                P.append(likelihood1(x, Z[:, 0:K_plus], M2, sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(num_objects - np.sum(Z[:,k])) - np.log(num_objects))\n",
    "            \n",
    "                P = np.exp(P - np.max(P))\n",
    "\n",
    "                # Sample from the conditional\n",
    "                if np.random.uniform() < P[0] / (P[0] + P[1]):\n",
    "                    Z[i, k] = 1\n",
    "                    M = M1\n",
    "                else:\n",
    "                    Z[i, k] = 0\n",
    "                    M = M2\n",
    "        \n",
    "            # Sample the number of new dishes for the current object\n",
    "            trun = np.zeros([1, 5])\n",
    "            alpha_N = alpha / num_objects\n",
    "\n",
    "\n",
    "################### ERROR IN DIMENSIONALITY RIGHT HERE ###########################        \n",
    "            for k_i in range(0, 5):\n",
    "                if k_i > 0:\n",
    "                    z = np.repeat(0, np.shape(Z)[0])\n",
    "                    Z = np.column_stack((Z, z))\n",
    "                    Z[i, K_plus:(K_plus+k_i)] = 1\n",
    "                M = np.linalg.inv(np.dot(Z[:, 0:(K_plus + k_i)].T, Z[:, 0:(K_plus+k_i)]) + (sigma_X**2/sigma_A**2) * np.eye(K_plus + k_i)) # MINUS ONE TO FIX DIMENSIONALITY MAY BE INCORRECT\n",
    "                trun[0, k_i] = k_i * np.log(alpha_N) - alpha_N - np.log(np.math.factorial(k_i)) + likelihood1(x, Z[:, 0:(K_plus + k_i)], M, sigma_A, sigma_X, K_plus + k_i, num_objects, object_dim)\n",
    "            Z[i, K_plus:(K_plus+4)] = 0\n",
    "            trun = np.exp(trun - np.max(trun))\n",
    "            trun = trun/np.sum(trun)\n",
    "            p = np.random.uniform()\n",
    "            t = 0\n",
    "            for k_i in range(0,5):\n",
    "                t = trun[0, k_i]\n",
    "                if p < t:\n",
    "                    new_dishes = k_i\n",
    "                    break\n",
    "            Z[i, K_plus:(K_plus + new_dishes)] = 1\n",
    "            K_plus = K_plus + new_dishes\n",
    "    \n",
    "        # Metropolis steps for sampling sigma_X and sigma_A\n",
    "        sigma_X = met_sigma_X(Z, K_plus, new_dishes, sigma_X, sigma_A, num_objects, object_dim, x)\n",
    "\n",
    "        sigma_A = met_sigma_A(Z, K_plus, new_dishes, sigma_X, sigma_A, num_objects, object_dim, x)\n",
    "\n",
    "        # Sample alpha from its conditional posterior\n",
    "        alpha = gamma.rvs(a = 1 + K_plus, scale = 1/(1+HN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%prun -q -D profiler.prof profiler(int(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pstats\n",
    "p = pstats.Stats('profiler.prof')\n",
    "p.print_stats()\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
