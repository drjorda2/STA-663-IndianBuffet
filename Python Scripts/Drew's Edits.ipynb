{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Performance of Improved Sampler vs. Original Sampler\n",
    "\n",
    "Now that we have improved our likelihood function, we compare the time to run our sampler using our optimized likelihood function and our original function. We also included the a Cythonized version of the sampler to improve performance further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cythonized Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import cython\n",
    "from libc.math cimport log\n",
    "\n",
    "cdef double pi = 3.141592653589793\n",
    "\n",
    "cdef sampleIBP_c(double alpha, long num_objects):  \n",
    "    cdef long i, j\n",
    "    cdef long t\n",
    "    cdef long x\n",
    "    cdef long y\n",
    "    cdef long K_plus\n",
    "\n",
    "    # Initializing storage for results\n",
    "    result = np.zeros([num_objects, 1000])\n",
    "\n",
    "    # Draw from the prior for alpha\n",
    "    t = np.random.poisson(alpha)\n",
    "    # Filling in first row of result matrix\n",
    "    result[0, 0:t] = np.ones(t) #changed form np.ones([1, t])\n",
    "    # Initializing K+\n",
    "    K_plus = t\n",
    "    \n",
    "    for i in range(1, num_objects):\n",
    "        for j in range(0, K_plus):\n",
    "            p = np.array([log(np.sum(result[0:i,j])) - log(i+1), \n",
    "                          log(i+1 - np.sum(result[0:i, j])) - log(i+1)])\n",
    "            p = np.exp(p - max(p))\n",
    "\n",
    "            if(np.random.uniform() < p[0]/np.sum(p)):\n",
    "                result[i, j] = 1\n",
    "            else:\n",
    "                result[i, j] = 0\n",
    "        t = np.random.poisson(alpha/(i+1))\n",
    "        x = K_plus + 1\n",
    "        y = K_plus + t\n",
    "        result[i, (x-1):y] = np.ones(t) #changed form np.ones([1, t])\n",
    "        K_plus = K_plus+t\n",
    "    result = result[:, 0:K_plus]\n",
    "\n",
    "    return list([result, K_plus])\n",
    "\n",
    "cdef double likelihood_opt_c(double[:,:] X, double[:, :] Z, double sigma_A, double sigma_X, \n",
    "                     long K_plus, long num_objects, long object_dim):\n",
    "\n",
    "    #Calculate M\n",
    "    cdef double[:, :] M = np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2)*np.eye(K_plus)\n",
    "    cdef double part1, part2, part3, part4, part5, total\n",
    "\n",
    "    part1 = (-1)*num_objects*(0.5*object_dim)*log(2*pi)\n",
    "    part2 = (-1)*(num_objects-K_plus)* object_dim *log(sigma_X) \n",
    "    part3 = (-1)*object_dim*K_plus*log(sigma_A) \n",
    "    part4 = (-1)*(0.5*object_dim)* log(np.linalg.det(M)) \n",
    "    part5 = (-1/(2*sigma_X**2)) * np.trace(np.dot(np.dot(X.T,(np.identity(num_objects) - np.dot(np.dot(Z,np.linalg.inv(M)),Z.T))),X))\n",
    "    total = part1+part2+part3+part4+part5\n",
    "\n",
    "    return(total)\n",
    "\n",
    "#This function samples new value of Z[i,k] and accepts by Metropolis\n",
    "cdef int Met_zval_c(double[:, :] data, double[:,:] Z, double sigma_X, double sigma_A, int K_plus, long num_objects, long object_dim, long i, long k):    \n",
    "    \n",
    "    cdef double[:]  P\n",
    "    cdef int new_sample\n",
    "    \n",
    "    P=np.zeros(2)\n",
    "\n",
    "    Z[i,k]=1\n",
    "    #Compute posterior density of new_sample\n",
    "    P[0] = likelihood_opt_c(data, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim) + log(np.sum(Z[:, k]) - Z[i,k]) - log(num_objects)\n",
    "\n",
    "    #Set new_sample to 0\n",
    "    Z[i,k]=0\n",
    "    #Computer posterior density of new_sample\n",
    "    P[1] = likelihood_opt_c(data, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim) + log(num_objects - np.sum(Z[:,k])) - log(num_objects)\n",
    "\n",
    "    P = np.exp(P - np.max(P))\n",
    "\n",
    "    if np.random.uniform(0,1) < (P[0]/(np.sum(P))):\n",
    "        new_sample = 1\n",
    "    else:\n",
    "        new_sample = 0\n",
    "    \n",
    "    return(new_sample)\n",
    "\n",
    "\n",
    "cdef int New_dishes_c(double[:,:] data, double[:,:] Z, double sigma_X, double  sigma_A, int K_plus, double alpha, long num_objects, long object_dim, long trunc_val, long i):\n",
    "    \n",
    "    trunc = np.zeros(trunc_val)\n",
    "    cdef double alpha_N = alpha/num_objects\n",
    "    cdef int k_i, new_dishes\n",
    "    cdef double[:,:] Z_temp, newcol\n",
    "    cdef double p, t\n",
    "    \n",
    "\n",
    "    for k_i in range(0,trunc_val):\n",
    "        Z_temp = Z\n",
    "        if k_i>0:\n",
    "            newcol = np.zeros((num_objects, k_i))\n",
    "            newcol[i,:] = 1 \n",
    "            Z_temp = np.column_stack((Z_temp, newcol))\n",
    "        trunc[k_i] = k_i * log(alpha_N) - alpha_N - log(np.math.factorial(k_i)) + likelihood_opt_c(data, Z_temp, sigma_A, sigma_X, K_plus+k_i, num_objects, object_dim)\n",
    "\n",
    "    trunc = np.exp(trunc - np.max(trunc))\n",
    "    trunc = trunc/np.sum(trunc)\n",
    "\n",
    "    p = np.random.uniform(0,1)\n",
    "    t = 0\n",
    "    new_dishes = 0\n",
    "\n",
    "    for k_i in range(0,trunc_val):\n",
    "        t = t + trunc[k_i]\n",
    "        if p < t:\n",
    "            new_dishes = k_i\n",
    "            break\n",
    "            \n",
    "    return(new_dishes)\n",
    "\n",
    "\n",
    "cdef Met_sigma_c(double[:,:] data, double[:,:] Z, double sigma_X, double sigma_A, int K_plus, long num_objects, long object_dim):\n",
    "    \n",
    "    cdef double sigma_X_new, sigma_A_new, lik_new_X, lik_new_A, acc_X, acc_A, sigma_X_val, sigma_A_val \n",
    "    \n",
    "    cdef double lik_curr = likelihood_opt_c(data, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim)\n",
    "\n",
    "    if np.random.uniform(0,1) < 0.5:\n",
    "        sigma_X_new = sigma_X - np.random.uniform(0,1)/20\n",
    "    else:\n",
    "        sigma_X_new = sigma_X + np.random.uniform(0,1)/20\n",
    "\n",
    "    lik_new_X = likelihood_opt_c(data, Z, sigma_A, sigma_X_new, K_plus, num_objects, object_dim)\n",
    "\n",
    "    acc_X = np.exp(min(0, lik_new_X - lik_curr))\n",
    "\n",
    "    if np.random.uniform(0,1) < 0.5:\n",
    "        sigma_A_new = sigma_A - np.random.uniform(0,1)/20\n",
    "    else:\n",
    "        sigma_A_new = sigma_A + np.random.uniform(0,1)/20\n",
    "\n",
    "    lik_new_A = likelihood_opt_c(data, Z, sigma_A_new, sigma_X, K_plus, num_objects, object_dim)\n",
    "\n",
    "    acc_A = np.exp(min(0, lik_new_A - lik_curr))\n",
    "    \n",
    "    sigma_X_val=0\n",
    "    sigma_A_val=0\n",
    "\n",
    "    if np.random.uniform(0,1) < acc_X:\n",
    "        sigma_X_val = sigma_X_new\n",
    "    else:\n",
    "        sigma_X_val = sigma_X\n",
    "    \n",
    "    if np.random.uniform(0,1) < acc_A:\n",
    "        sigma_A_val = sigma_A_new\n",
    "    else:\n",
    "        sigma_A_val = sigma_A\n",
    "        \n",
    "    return list([sigma_X_val, sigma_A_val])\n",
    "\n",
    "\n",
    "\n",
    "def sampler_opt_c(double[:,:] data, long E=1000, long K_inf = 20, double sigma_X = 1, double sigma_A = 1, double alpha = 1, int trunc_val=5):\n",
    "    \n",
    "    cdef long num_objects= np.shape(data)[0]\n",
    "    cdef long object_dim = np.shape(data)[1]\n",
    "    \n",
    "    #Set storage arrays for sampled parameters\n",
    "    cdef double[:, :, :] chain_Z = np.zeros([E, num_objects, K_inf])\n",
    "    cdef double[:, :] chain_K = np.zeros([E, 1])\n",
    "    cdef double[:, :] chain_sigma_X = np.zeros([E, 1])\n",
    "    cdef double[:, :] chain_sigma_A = np.zeros([E, 1])\n",
    "    cdef double[:, :] chain_alpha = np.zeros([E, 1])\n",
    "    \n",
    "    cdef double HN\n",
    "    cdef long i, e, j1, j2, k, k_i, TEMP\n",
    "\n",
    "    #Initialize parameter values\n",
    "    \n",
    "    K_plus = 0\n",
    "    while K_plus == 0:\n",
    "        [Z, K_plus] = sampleIBP_c(alpha, num_objects)\n",
    "\n",
    "    #Compute Harmonic Number\n",
    "    HN = 0\n",
    "    for i in range(0, num_objects):\n",
    "        HN = HN + 1.0/(i+1)\n",
    "\n",
    "    for e in range(0, E):\n",
    "        #Store sampled values\n",
    "        for j1 in range(num_objects):\n",
    "            for j2 in range(K_plus):\n",
    "                chain_Z[e, j1, j2] = Z[j1, j2]\n",
    "        chain_K[e] = K_plus\n",
    "        chain_sigma_X[e] = sigma_X\n",
    "        chain_sigma_A[e] = sigma_A\n",
    "        chain_alpha[e] = alpha\n",
    "\n",
    "        #if (e%100==0):\n",
    "        #    print(e)\n",
    "        #print(\"At iteration\", e, \": K_plus is\", K_plus, \", alpha is\", alpha) \n",
    "\n",
    "        #Generate a new value for Z[i,k] and accept by Metropolis\n",
    "        for i in range(0, num_objects):\n",
    "            #First we remove singular features if any\n",
    "            for k in range(0, K_plus):\n",
    "                if (k>=K_plus):\n",
    "                    break\n",
    "                if(Z[i, k] > 0):\n",
    "                    if (np.sum(Z[:, k]) - Z[i, k]) <= 0: \n",
    "                        Z[i, k] = 0\n",
    "                        Z[:, k:(K_plus - 1)] = Z[:, (k+1):K_plus]\n",
    "                        K_plus = K_plus - 1\n",
    "                        Z = Z[:, 0:K_plus]\n",
    "                        continue\n",
    "                #Compute conditional distribution for current cell\n",
    "                TEMP = Met_zval_c(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim, i, k)\n",
    "                Z[i,k] = TEMP\n",
    "\n",
    "\n",
    "            #Sample new dishes by Metropolis\n",
    "            new_dishes = New_dishes_c(data, Z, sigma_X, sigma_A, K_plus, alpha, num_objects, object_dim, trunc_val,i)\n",
    "\n",
    "            if(new_dishes > 0):\n",
    "                newcol = np.zeros((num_objects, new_dishes))\n",
    "                newcol[i,:] = 1\n",
    "                Z = np.column_stack((Z, newcol))\n",
    "            K_plus = K_plus + new_dishes\n",
    "\n",
    "        #Sample sigma_X and sigma_A through Metropolis\n",
    "        [sigma_X, sigma_A] = Met_sigma_c(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim)\n",
    "        #Sample alpha via Gibbs\n",
    "        alpha = np.random.gamma(1 + K_plus, 1/(1+HN))\n",
    "    \n",
    "    print(\"Complete\")\n",
    "    return list([chain_Z, chain_K, chain_sigma_X, chain_sigma_A, chain_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "# Setting values to use in likelihood function comparisons\n",
    "num_objects = image_data.shape[0]\n",
    "object_dim = image_data.shape[1]\n",
    "\n",
    "alpha = 1\n",
    "\n",
    "Z, K_plus = sampleIBP(alpha,num_objects)\n",
    "\n",
    "# Time the original likelihood function\n",
    "loops = 1\n",
    "time_sampler=np.zeros(loops)\n",
    "for l in range(loops):\n",
    "    t0=time.time()\n",
    "    Sampler(image_data, num_objects, object_dim, E=100,  K_inf = 20, sigma_X = 1, sigma_A = 1, alpha = 1, trunc_val=5)\n",
    "    t1=time.time()\n",
    "    time_sampler[l]=t1-t0\n",
    "mean_time_sampler = round(np.mean(time_sampler),7)\n",
    "\n",
    "\n",
    "np.random.seed(1234)\n",
    "# Time the optimized likelihood function\n",
    "time_sampler_opt = np.zeros(loops)\n",
    "for l in range(loops):\n",
    "    t0 = time.time()\n",
    "    sampler_opt(image_data, num_objects, object_dim, E=100,  K_inf = 20, sigma_X = 1, sigma_A = 1, alpha = 1, trunc_val=5)\n",
    "    t1 = time.time()\n",
    "    time_sampler_opt[l] = t1-t0\n",
    "mean_time_sampler_opt = round(np.mean(time_sampler_opt), 7)\n",
    "\n",
    "np.random.seed(1234)\n",
    "# Time the optimized likelihood function\n",
    "time_sampler_opt_c = np.zeros(loops)\n",
    "for l in range(loops):\n",
    "    t0 = time.time()\n",
    "    sampler_opt_c(image_data, E=100, K_inf = 20, sigma_X = 1, sigma_A = 1, alpha = 1, trunc_val=5)\n",
    "    t1 = time.time()\n",
    "    time_sampler_opt_c[l] = t1-t0\n",
    "mean_time_sampler_opt_c = round(np.mean(time_sampler_opt_c), 7)\n",
    "\n",
    "time_array = np.array([mean_time_sampler, mean_time_sampler_opt, mean_time_sampler_opt_c])\n",
    "\n",
    "cols = [\"Time\"]\n",
    "index = [\"Original Sampler\", \"Optimized Sampler\", \"Cythonized Sampler\"]\n",
    "\n",
    "time_df = pd.DataFrame(time_array, columns = cols, index = index)\n",
    "time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see when run for 100 iterations, we get a significant speed up for our optimized likelihood sampler over our original likelihood sampler. We are not as fortunate with our Cythonized code, as this does not seem to introduce any improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "====\n",
    "\n",
    "[1] Thomas Griffiths and Zoubin Ghahramani. Infinite latent feature models and the Indian buffet process. 2005.\n",
    "\n",
    "[2] Thomas Griffiths and Zoubin Ghahramani. Infinite latent feature models and the Indian buffet process. technical report 2005-     001. 2005\n",
    "\n",
    "[3] Ilker Yildirim. http://www.mit.edu/ilkery/.\n",
    "\n",
    "[4] Ilker Yildirim. Bayesian statistics: Indian buffet process. 2012.\n",
    "\n",
    "[5] Dipesh Gautam. Indian Buffet Process and its application in the Infinite Latent Feature Model. 2015\n",
    "\n",
    "[6] Christine Chai. Implementation of the Indian Buffet Process. 2015\n",
    "\n",
    "[7] Radhika Anand. Infinite Latent Feature Models and the Indian Buffet Process. 2015"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
