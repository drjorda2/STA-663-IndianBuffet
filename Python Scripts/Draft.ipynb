{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Uploading required packages\n",
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import uniform as unif\n",
    "from scipy.stats import multivariate_normal as mvtnorm\n",
    "from scipy.stats import bernoulli\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indian Buffet Process Function\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indian Buffet Process Function\n",
    "\n",
    "def sampleIBP(alpha, num_objects):  \n",
    "    # Initializing storage for results\n",
    "    result = np.zeros([num_objects, 1000])\n",
    "    # Draw from the prior for alpha\n",
    "    t = np.random.poisson(alpha)\n",
    "    # Filling in first row of result matrix\n",
    "    result[0, 0:t] = np.ones(t) #changed form np.ones([1, t])\n",
    "    # Initializing K+\n",
    "    K_plus = t\n",
    "    \n",
    "    for i in range(1, num_objects):\n",
    "        for j in range(0, K_plus):\n",
    "            p = np.array([np.log(np.sum(result[0:i,j])) - np.log(i+1), \n",
    "                          np.log(i+1 - np.sum(result[0:i, j])) - np.log(i+1)])\n",
    "            p = np.exp(p - max(p))\n",
    "\n",
    "            if(np.random.uniform() < p[0]/np.sum(p)):\n",
    "                result[i, j] = 1\n",
    "            else:\n",
    "                result[i, j] = 0\n",
    "        t = np.random.poisson(alpha/(i+1))\n",
    "        x = K_plus + 1\n",
    "        y = K_plus + t\n",
    "        result[i, (x-1):y] = np.ones(t) #changed form np.ones([1, t])\n",
    "        K_plus = K_plus+t\n",
    "    result = result[:, 0:K_plus]\n",
    "    \n",
    "    return list([result, K_plus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Simulation\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Simulation\n",
    "\n",
    "#Latent Features\n",
    "W = np.array([[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]])\n",
    "\n",
    "\n",
    "#Each image in our simulated data set is the superposition of four base images#\n",
    "# Number of images/ data points\n",
    "num_objects=100\n",
    "\n",
    "#Dimension of image (6x6)\n",
    "object_dim  = 6*6\n",
    "\n",
    "#Covariance matrix for images/ white noise\n",
    "sigma_x_orig = 0.5\n",
    "I = sigma_x_orig * np.identity(object_dim)\n",
    "\n",
    "#z_i - binary feature matrix (1 x 4) - each entry set to 1 with probability 0.5 and 0 otherwise#\n",
    "#x is data variable - each row correspondes to a superimposed built from a random combination of latent features#\n",
    "#with white noise added - x is built with multivariate gaussian#\n",
    "image_data = np.zeros((100,36))\n",
    "z_org = np.zeros((100,4))\n",
    "\n",
    "for i in range(0,num_objects):\n",
    "    z_org[i,:] = np.array([bernoulli.rvs(p=0.5, size=4)])\n",
    "    image_data[i,:] = np.dot(z_org[i,:],W) + np.random.normal(0,1, (1,object_dim)).dot(I) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation\n",
    "====\n",
    "\n",
    "We will be using an infinite Gaussian binary latent feature model with an Indian Buffet Process prior to model the number of latent features present in our simulated image dataset. We follow the example as discussed in Griffiths and Ghahramani(2005).\n",
    "\n",
    "We begin by defining a binary feature ownership matrix $\\textbf{Z}$ that represents whether or not each feature is present given the observations $\\textbf{X}$. Therefore each D-dimensional object $\\textbf{x_i}$ has a Gaussian distribution:\n",
    "$$x_i \\sim Normal(z_i A, \\Sigma_X)$$\n",
    "\n",
    "Where $\\textbf{A}$ is a $\\textit{K x D}$ matrix of weights representing the $\\textit{K}$ latent features. The noise introduced into our simulated images is represented by the covariance $\\Sigma_X$. The prior on $\\textbf A$ is matrix Gaussian as well with mean 0 and covariance $\\Sigma_A$. We then integrate out $\\textbf A$ to generate the data likelihood:\n",
    "\n",
    "$$P(X|Z,\\sigma_X, \\sigma_A) = \\frac{1}{(2 \\pi)^{ND/2} (\\sigma_X)^{(N-K)D}(\\sigma_A)^{KD}(|Z^TZ+\\frac{\\sigma_X^2}{\\sigma_A^2}I|)^{D/2}} exp\\{-\\frac{1}{2\\sigma_X^2}tr(X^T(I-Z(Z^TZ+\\frac{\\sigma_X^2}{\\sigma_A^2}I)^{-1}Z^T)X)\\}$$\n",
    "\n",
    "### Gibbs Sampling and Metropolis-Hastings Algorithm\n",
    "\n",
    "We now implement both Gibbs Sampling and Metropolis-Hastings to perform inference on our image dataset.\n",
    "\n",
    "#### Parameters of Interest\n",
    "\n",
    "We have five parameters of interest in our model that we need to update throughout our MCMC process.\n",
    "\n",
    "1.) Z: feature ownernship matrix\n",
    "\n",
    "2.) $K_+$: number of new latent features\n",
    "\n",
    "3.) $\\alpha$ parameter for  $K_+$\n",
    "\n",
    "4.) $\\sigma_x$\n",
    "\n",
    "5.) $\\sigma_A$\n",
    "\n",
    "We are able to find known full conditional distributions for Z, $K_+$, and $\\alpha$, so we will update them using Gibbs Sampling. We will update $\\sigma_X$ and $\\sigma_A$ using random-walk Metropolis-Hastings.\n",
    "\n",
    "#### Prior Distributions\n",
    "\n",
    "We begin by setting a prior on on our parameter controlling $K_+$, $\\alpha$:\n",
    "\n",
    "$$\\alpha \\sim Gamma(1,1)$$\n",
    "\n",
    "We then set a prior on our latent feature binary matrix Z using the Indian Buffet Process prior:\n",
    "\n",
    "$$P(z_{ik} = 1 | \\textbf{z}_{-i,k}) = \\dfrac{n_{-i,k}}{N}$$\n",
    "\n",
    "Finally, we set a Poisson prior on the number of latent features $K_+$:\n",
    "\n",
    "$$K_+ \\sim Poisson(\\dfrac{\\alpha}{N})$$\n",
    "\n",
    "#### Full Conditional Distributions Used  for Gibbs Sampling\n",
    "\n",
    "Now that we have defined our likelihood and selected our priors, we will now define our full conditional distributions for Z, $K_+$, and $\\alpha$ to be used in Gibbs Sampling. Beginning with Z, we find the full conditional distribution to be:\n",
    "\n",
    "$$P(z_{ik}|X,Z_{-(i,k),},\\sigma_X,\\sigma_A) \\propto  P(X|Z,\\sigma_X, \\sigma_A) * P(z_{ik}=1|\\textbf{z}_{-i,k})$$\n",
    "\n",
    "To sample the number of new features $K_+$ for observation $i$, we use our data likelihood and our Poisson$(\\dfrac{\\alpha}{N})$ prior for $K_+$ and truncate this distribution for a range of values of $K_+$ up to 4 new features. We then use this to compute the probability distribution for $K_+$ and sample the number of new features from this distribution.\n",
    "\n",
    "#### Metropolis-Hasting Updates\n",
    "\n",
    "To update $\\sigma_X$ and $\\sigma_A$, we use random walk Metropolis-Hastings steps. For $\\sigma_X$, we generate a random value from a Uniform(-.05, .05) distribution and add this value to our current value of $\\sigma_X$ to get $\\sigma_X^*$. We then accept our new value of $\\sigma_X$ with probability:\n",
    "\n",
    "$$p = min(1, \\dfrac{P(X|Z, \\sigma_X^*, \\sigma_A}{P(X|Z, \\sigma_X, \\sigma_A})$$\n",
    "\n",
    "To update $\\sigma_A$, we follow the same proceedure as with $\\sigma_X$, replacing $\\sigma_X$ with $\\sigma_A$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiling & Optimization\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Used within the Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood Function\n",
    "\n",
    "The likelihood function is used to compute $P(X|Z,\\sigma_X, \\sigma_A)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Likelihood function used to compute P(X | Z, sigma_X, sigma_A)\n",
    "def likelihood(X, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim):\n",
    "    part1 = (-1)*num_objects*(0.5*object_dim)*np.log(2*np.pi)\n",
    "    part2 = (-1)*(num_objects-K_plus)* object_dim *np.log(sigma_X) \n",
    "    part3 = (-1)*object_dim*K_plus*np.log(sigma_A) \n",
    "    part4 = (-1)*(0.5*object_dim)* np.log(np.linalg.det((np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2)*np.eye(K_plus)))) \n",
    "    part5 = (-1/(2*sigma_X**2)) * np.trace(np.dot(np.dot(X.T,(np.identity(num_objects) - np.dot(np.dot(Z,M),Z.T))),X))\n",
    "    total = part1+part2+part3+part4+part5\n",
    "    return(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mcalc Function\n",
    "\n",
    "The Mcalc function is used to compute $(Z^T Z + (\\dfrac{\\sigma_X^2}{\\sigma_A^2}) * I)^{-1}$ which is used in several calculations throughout the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Mcalc(Z, sigma_X, sigma_A, K_plus):\n",
    "    M = np.linalg.inv(np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2) * np.eye(K_plus))\n",
    "    return(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Met_zval Function\n",
    "\n",
    "The Met_zval function is used to update $z_{i,k}$ using Gibbs Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function samples new value of Z[i,k] using Gibbs Sampling\n",
    "def Met_zval(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim, i, k):    \n",
    "    \n",
    "    P=np.zeros(2)\n",
    "\n",
    "    Z[i,k]=1\n",
    "    #Compute posterior density of new_sample\n",
    "    M = Mcalc(Z, sigma_X, sigma_A, K_plus)\n",
    "    P[0] = likelihood(data, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(np.sum(Z[:, k]) - Z[i,k]) - np.log(num_objects)\n",
    "\n",
    "    #Set new_sample to 0\n",
    "    Z[i,k]=0\n",
    "    #Computer posterior density of new_sample\n",
    "    M = Mcalc(Z, sigma_X, sigma_A, K_plus)\n",
    "    P[1] = likelihood(data, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim) + np.log(num_objects - np.sum(Z[:,k])) - np.log(num_objects)\n",
    "\n",
    "    P = np.exp(P - np.max(P))\n",
    "\n",
    "    if np.random.uniform(0,1) < (P[0]/(np.sum(P))):\n",
    "        new_sample = 1\n",
    "    else:\n",
    "        new_sample = 0\n",
    "    \n",
    "    return(new_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New_dishes Function\n",
    "\n",
    "The New_dishes function samples the number of new features for observation $i$ using Gibbs Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def New_dishes(data, Z, sigma_X, sigma_A, K_plus, alpha, num_objects, object_dim, trunc_val,i):\n",
    "    trunc = np.zeros(trunc_val)\n",
    "    alpha_N = alpha/num_objects\n",
    "\n",
    "    for k_i in range(0,trunc_val):\n",
    "        Z_temp = Z\n",
    "        if k_i>0:\n",
    "            newcol = np.zeros((num_objects, k_i))\n",
    "            newcol[i,:] = 1 \n",
    "            Z_temp = np.column_stack((Z_temp, newcol))\n",
    "        M = Mcalc(Z_temp, sigma_X, sigma_A, K_plus+k_i)\n",
    "        trunc[k_i] = k_i * np.log(alpha_N) - alpha_N - np.log(np.math.factorial(k_i)) + likelihood(data, Z_temp, M, sigma_A, sigma_X, K_plus+k_i, num_objects, object_dim)\n",
    "\n",
    "    trunc = np.exp(trunc - np.max(trunc))\n",
    "    trunc = trunc/np.sum(trunc)\n",
    "\n",
    "    p = np.random.uniform(0,1)\n",
    "    t = 0\n",
    "    new_dishes = 0\n",
    "\n",
    "    for k_i in range(0,trunc_val):\n",
    "        t = t + trunc[k_i]\n",
    "        if p < t:\n",
    "            new_dishes = k_i\n",
    "            break\n",
    "            \n",
    "    return(new_dishes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Met_sigma Function\n",
    "\n",
    "The Met_sigma function updates both $\\sigma_X$ and $\\sigma_A$ using random-walk Metropolis-Hastings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Met_sigma(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim):\n",
    "    \n",
    "    M = Mcalc(Z, sigma_X, sigma_A, K_plus)  \n",
    "    lik_curr = likelihood(data, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim)\n",
    "\n",
    "    if np.random.uniform(0,1) < 0.5:\n",
    "        sigma_X_new = sigma_X - np.random.uniform(0,1)/20\n",
    "    else:\n",
    "        sigma_X_new = sigma_X + np.random.uniform(0,1)/20\n",
    "\n",
    "    M = Mcalc(Z, sigma_X_new, sigma_A, K_plus)\n",
    "    lik_new_X = likelihood(data, Z, M, sigma_A, sigma_X_new, K_plus, num_objects, object_dim)\n",
    "\n",
    "    acc_X = np.exp(min(0, lik_new_X - lik_curr))\n",
    "\n",
    "    if np.random.uniform(0,1) < 0.5:\n",
    "        sigma_A_new = sigma_A - np.random.uniform(0,1)/20\n",
    "    else:\n",
    "        sigma_A_new = sigma_A + np.random.uniform(0,1)/20\n",
    "\n",
    "    M = Mcalc(Z, sigma_X, sigma_A_new, K_plus)\n",
    "    lik_new_A = likelihood(data, Z, M, sigma_A_new, sigma_X, K_plus, num_objects, object_dim)\n",
    "\n",
    "    acc_A = np.exp(min(0, lik_new_A - lik_curr))\n",
    "    \n",
    "    sigma_X_val=0\n",
    "    sigma_A_val=0\n",
    "\n",
    "    if np.random.uniform(0,1) < acc_X:\n",
    "        sigma_X_val = sigma_X_new\n",
    "    else:\n",
    "        sigma_X_val = sigma_X\n",
    "    \n",
    "    if np.random.uniform(0,1) < acc_A:\n",
    "        sigma_A_val = sigma_A_new\n",
    "    else:\n",
    "        sigma_A_val = sigma_A\n",
    "        \n",
    "    return list([sigma_X_val, sigma_A_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampler Function\n",
    "\n",
    "The Sampler function combines all of the above functions to run the entirety of our MCMC algorithm. It outputs the posterior distibutions for Z, $K_+$, $\\sigma_X$, $\\sigma_A$, and $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Sampler(data, num_objects, object_dim, E=1000,  K_inf = 20, sigma_X = 1, sigma_A = 1, alpha = 1, trunc_val=5):\n",
    "    #Set storage arrays for sampled parameters\n",
    "    chain_Z = np.zeros([E, num_objects, K_inf])\n",
    "    chain_K = np.zeros([E, 1])\n",
    "    chain_sigma_X = np.zeros([E, 1])\n",
    "    chain_sigma_A = np.zeros([E, 1])\n",
    "    chain_alpha = np.zeros([E, 1])\n",
    "\n",
    "    #Initialize parameter values\n",
    "    num_object= np.shape(data)[0]\n",
    "    object_dim = np.shape(data)[1]\n",
    "\n",
    "    K_plus = 0\n",
    "    while K_plus == 0:\n",
    "        [Z, K_plus] = sampleIBP(alpha, num_objects)\n",
    "\n",
    "    #Compute Harmonic Number\n",
    "    HN = 0\n",
    "    for i in range(0, num_objects):\n",
    "        HN = HN + 1.0/(i+1)\n",
    "\n",
    "    for e in range(0, E):\n",
    "        #Store sampled values\n",
    "        chain_Z[e, :, 0:K_plus] = Z[:, 0:K_plus]\n",
    "        chain_K[e] = K_plus\n",
    "        chain_sigma_X[e] = sigma_X\n",
    "        chain_sigma_A[e] = sigma_A\n",
    "        chain_alpha[e] = alpha\n",
    "\n",
    "        #if (e%100==0):\n",
    "        #    print(e)\n",
    "        print(\"At iteration\", e, \": K_plus is\", K_plus, \", alpha is\", alpha) \n",
    "\n",
    "        #Generate a new value for Z[i,k] and accept by Metropolis\n",
    "        for i in range(0, num_objects):\n",
    "            #First we remove singular features if any\n",
    "            for k in range(0, K_plus):\n",
    "                if (k>=K_plus):\n",
    "                    break\n",
    "                if(Z[i, k] > 0):\n",
    "                    if (np.sum(Z[:, k]) - Z[i, k]) <= 0: \n",
    "                        Z[i, k] = 0\n",
    "                        Z[:, k:(K_plus - 1)] = Z[:, (k+1):K_plus]\n",
    "                        K_plus = K_plus - 1\n",
    "                        Z = Z[:, 0:K_plus]\n",
    "                        continue\n",
    "                #Compute conditional distribution for current cell\n",
    "                Z[i,k] = Met_zval(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim, i, k)\n",
    "\n",
    "            #Sample new dishes by Metropolis\n",
    "            new_dishes = New_dishes(data, Z, sigma_X, sigma_A, K_plus, alpha, num_objects, object_dim, trunc_val,i)\n",
    "\n",
    "            if(new_dishes > 0):\n",
    "                newcol = np.zeros((num_objects, new_dishes))\n",
    "                newcol[i,:] = 1\n",
    "                Z = np.column_stack((Z, newcol))\n",
    "            K_plus = K_plus + new_dishes\n",
    "\n",
    "        #Sample sigma_X and sigma_A through Metropolis\n",
    "        [sigma_X, sigma_A] = Met_sigma(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim)\n",
    "        #Sample alpha via Gibbs\n",
    "        alpha = np.random.gamma(1 + K_plus, 1/(1+HN))\n",
    "    \n",
    "    print(\"Complete\")\n",
    "    return list([chain_Z, chain_K, chain_sigma_X, chain_sigma_A, chain_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0 : K_plus is 5 , alpha is 1\n",
      "At iteration 1 : K_plus is 2 , alpha is 1.491787700240985\n",
      "At iteration 2 : K_plus is 2 , alpha is 0.5755998482400341\n",
      "At iteration 3 : K_plus is 2 , alpha is 0.22085176428487893\n",
      "At iteration 4 : K_plus is 2 , alpha is 0.5890523479952683\n",
      "At iteration 5 : K_plus is 2 , alpha is 0.2585216472795352\n",
      "At iteration 6 : K_plus is 1 , alpha is 0.650067715283336\n",
      "At iteration 7 : K_plus is 1 , alpha is 0.6686107997241004\n",
      "At iteration 8 : K_plus is 1 , alpha is 0.15944916040307874\n",
      "At iteration 9 : K_plus is 1 , alpha is 0.5371704847408318\n",
      "At iteration 10 : K_plus is 1 , alpha is 0.2970079849217686\n",
      "At iteration 11 : K_plus is 1 , alpha is 0.2035018579644125\n",
      "At iteration 12 : K_plus is 1 , alpha is 0.17054524193403517\n",
      "At iteration 13 : K_plus is 1 , alpha is 0.03805691883740289\n",
      "At iteration 14 : K_plus is 1 , alpha is 0.0741276470726093\n",
      "At iteration 15 : K_plus is 1 , alpha is 0.09752448508972106\n",
      "At iteration 16 : K_plus is 1 , alpha is 0.5028424369727091\n",
      "At iteration 17 : K_plus is 1 , alpha is 0.9969470789099117\n",
      "At iteration 18 : K_plus is 1 , alpha is 0.21723381824221913\n",
      "At iteration 19 : K_plus is 1 , alpha is 0.24828303253359724\n",
      "At iteration 20 : K_plus is 1 , alpha is 0.2590168602206595\n",
      "At iteration 21 : K_plus is 1 , alpha is 0.7803875302430678\n",
      "At iteration 22 : K_plus is 1 , alpha is 0.3612036639508932\n",
      "At iteration 23 : K_plus is 1 , alpha is 0.058459733018375544\n",
      "At iteration 24 : K_plus is 1 , alpha is 0.15264821147813826\n",
      "At iteration 25 : K_plus is 1 , alpha is 0.09446548209706566\n",
      "At iteration 26 : K_plus is 1 , alpha is 0.3685816412375746\n",
      "At iteration 27 : K_plus is 1 , alpha is 0.3259246264224325\n",
      "At iteration 28 : K_plus is 1 , alpha is 0.0808726449561632\n",
      "At iteration 29 : K_plus is 1 , alpha is 0.1591408686542417\n",
      "At iteration 30 : K_plus is 1 , alpha is 0.2034482824585073\n",
      "At iteration 31 : K_plus is 1 , alpha is 0.06347709485245456\n",
      "At iteration 32 : K_plus is 1 , alpha is 0.11940428450495756\n",
      "At iteration 33 : K_plus is 1 , alpha is 0.032708064033255435\n",
      "At iteration 34 : K_plus is 1 , alpha is 0.06874049192021736\n",
      "At iteration 35 : K_plus is 1 , alpha is 0.20109103667329922\n",
      "At iteration 36 : K_plus is 1 , alpha is 0.39666533910158297\n",
      "At iteration 37 : K_plus is 1 , alpha is 0.16505913079410922\n",
      "At iteration 38 : K_plus is 1 , alpha is 0.7060847742870041\n",
      "At iteration 39 : K_plus is 1 , alpha is 0.10889234261208942\n",
      "At iteration 40 : K_plus is 1 , alpha is 0.4114913406767846\n",
      "At iteration 41 : K_plus is 1 , alpha is 0.13421105202869535\n",
      "At iteration 42 : K_plus is 1 , alpha is 0.19595522166312807\n",
      "At iteration 43 : K_plus is 1 , alpha is 0.13668750096203153\n",
      "At iteration 44 : K_plus is 1 , alpha is 0.14092838837195987\n",
      "At iteration 45 : K_plus is 1 , alpha is 0.0685361726234076\n",
      "At iteration 46 : K_plus is 1 , alpha is 0.09323481337827179\n",
      "At iteration 47 : K_plus is 1 , alpha is 0.33904592940031414\n",
      "At iteration 48 : K_plus is 1 , alpha is 0.35591354407085574\n",
      "At iteration 49 : K_plus is 1 , alpha is 0.4300603929189889\n",
      "At iteration 50 : K_plus is 1 , alpha is 0.48735102111814993\n",
      "At iteration 51 : K_plus is 2 , alpha is 0.08469636577652548\n",
      "At iteration 52 : K_plus is 2 , alpha is 0.2864547669627781\n",
      "At iteration 53 : K_plus is 2 , alpha is 0.15631708429320354\n",
      "At iteration 54 : K_plus is 2 , alpha is 0.13736710593315368\n",
      "At iteration 55 : K_plus is 2 , alpha is 0.17134774440363854\n",
      "At iteration 56 : K_plus is 3 , alpha is 0.8739654138637598\n",
      "At iteration 57 : K_plus is 3 , alpha is 0.6447590662453108\n",
      "At iteration 58 : K_plus is 4 , alpha is 0.5846743785396908\n",
      "At iteration 59 : K_plus is 4 , alpha is 2.1719444009041378\n",
      "At iteration 60 : K_plus is 4 , alpha is 0.923562423245525\n",
      "At iteration 61 : K_plus is 4 , alpha is 0.4707170621680315\n",
      "At iteration 62 : K_plus is 4 , alpha is 0.8000594200202995\n",
      "At iteration 63 : K_plus is 5 , alpha is 1.0456576826178061\n",
      "At iteration 64 : K_plus is 5 , alpha is 0.6048827189964092\n",
      "At iteration 65 : K_plus is 6 , alpha is 1.3366933756357442\n",
      "At iteration 66 : K_plus is 6 , alpha is 1.5645994927990379\n",
      "At iteration 67 : K_plus is 6 , alpha is 1.1857805984634011\n",
      "At iteration 68 : K_plus is 6 , alpha is 0.8536175620685452\n",
      "At iteration 69 : K_plus is 6 , alpha is 0.4140615427279169\n",
      "At iteration 70 : K_plus is 6 , alpha is 1.2129598450842527\n",
      "At iteration 71 : K_plus is 6 , alpha is 0.8531209880613837\n",
      "At iteration 72 : K_plus is 5 , alpha is 0.6484302127300294\n",
      "At iteration 73 : K_plus is 6 , alpha is 1.02008226327423\n",
      "At iteration 74 : K_plus is 7 , alpha is 0.6707648376965445\n",
      "At iteration 75 : K_plus is 7 , alpha is 0.794555445662207\n",
      "At iteration 76 : K_plus is 6 , alpha is 0.9886774577858418\n",
      "At iteration 77 : K_plus is 6 , alpha is 1.5472965033025912\n",
      "At iteration 78 : K_plus is 5 , alpha is 1.331628576799814\n",
      "At iteration 79 : K_plus is 7 , alpha is 1.797889628468733\n",
      "At iteration 80 : K_plus is 8 , alpha is 1.1504285154320975\n",
      "At iteration 81 : K_plus is 5 , alpha is 0.9435038698345176\n",
      "At iteration 82 : K_plus is 6 , alpha is 1.4206957058680754\n",
      "At iteration 83 : K_plus is 6 , alpha is 1.2445966533204884\n",
      "At iteration 84 : K_plus is 5 , alpha is 1.2840374850065828\n",
      "At iteration 85 : K_plus is 5 , alpha is 0.7341905214964854\n",
      "At iteration 86 : K_plus is 5 , alpha is 0.8397355734578189\n",
      "At iteration 87 : K_plus is 6 , alpha is 0.9747866121225645\n",
      "At iteration 88 : K_plus is 6 , alpha is 1.0041253360672557\n",
      "At iteration 89 : K_plus is 6 , alpha is 1.1423479119610378\n",
      "At iteration 90 : K_plus is 5 , alpha is 0.78537854816171\n",
      "At iteration 91 : K_plus is 5 , alpha is 1.2875542106840538\n",
      "At iteration 92 : K_plus is 5 , alpha is 0.7157566015849017\n",
      "At iteration 93 : K_plus is 6 , alpha is 1.6501239847481557\n",
      "At iteration 94 : K_plus is 6 , alpha is 1.1721125451718861\n",
      "At iteration 95 : K_plus is 6 , alpha is 1.337665842931395\n",
      "At iteration 96 : K_plus is 5 , alpha is 0.5781471303638908\n",
      "At iteration 97 : K_plus is 7 , alpha is 2.0059053763397063\n",
      "At iteration 98 : K_plus is 6 , alpha is 0.9352983895842459\n",
      "At iteration 99 : K_plus is 8 , alpha is 0.7938568723163032\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "dog = Sampler(image_data, num_objects, object_dim, E=100,  K_inf = 20, sigma_X = 1, sigma_A = 1, alpha = 1, trunc_val=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling Our Sampler\n",
    "\n",
    "In order to test the efficiency our code, we profile our Sampler function to find any potential bottlenecks in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n",
      "Fri Apr 29 19:58:06 2016    profiling_unoptimized\n",
      "\n",
      "         9027307 function calls in 61.586 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 65 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   61.586   61.586 {built-in method builtins.exec}\n",
      "        1    0.000    0.000   61.586   61.586 <string>:1(<module>)\n",
      "        1    0.264    0.264   61.586   61.586 <ipython-input-9-d51565b6ccfe>:1(Sampler)\n",
      "   122948    8.445    0.000   44.037    0.000 <ipython-input-4-cf0b3993be3a>:2(likelihood)\n",
      "    36324    1.702    0.000   36.025    0.001 <ipython-input-6-7997664e2c7c>:2(Met_zval)\n",
      "    10000    1.412    0.000   24.966    0.002 <ipython-input-7-3b4495a7a593>:1(New_dishes)\n",
      "   737688   24.683    0.000   24.683    0.000 {built-in method numpy.core.multiarray.dot}\n",
      "   122948    1.972    0.000   11.339    0.000 <ipython-input-5-c62396fa41c4>:1(Mcalc)\n",
      "   122948    3.927    0.000    6.649    0.000 linalg.py:458(inv)\n",
      "   122948    2.535    0.000    4.973    0.000 linalg.py:1723(det)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Profiling our code for 100 iterations of our Sampler Function\n",
    "import cProfile\n",
    "import re\n",
    "np.random.seed(1234)\n",
    "cProfile.run('Sampler(image_data, num_objects, object_dim, E=100,  K_inf = 20, sigma_X = 1, sigma_A = 1, alpha = 1, trunc_val=5)', \"profiling_unoptimized\")\n",
    "\n",
    "# Displaying results of our profiling\n",
    "import pstats\n",
    "p = pstats.Stats('profiling_unoptimized')\n",
    "p.strip_dirs().sort_stats(\"cumulative\").print_stats(10)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from our profiling output above that the biggest bottleneck within our code is our likelihood function. We will focus our optimization efforts on this function going forward. We notice that we call the Mcalc function along with likelihood function in tandem several times throughout our Sampler function. To reduce the number of calls and to speed up our Sampler function, we move the calculation of the M matrix into our likelihood function. This leads us to a new function we call likelihood_opt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood_opt Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Optimized likelihood\n",
    "# This function return the log likelihood\n",
    "def likelihood_opt(X, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim):\n",
    "    #Calculate M\n",
    "    M = np.dot(Z.T, Z) + (sigma_X**2/sigma_A**2)*np.eye(K_plus)\n",
    "    \n",
    "    part1 = (-1)*num_objects*(0.5*object_dim)*np.log(2*np.pi)\n",
    "    part2 = (-1)*(num_objects-K_plus)* object_dim *np.log(sigma_X) \n",
    "    part3 = (-1)*object_dim*K_plus*np.log(sigma_A) \n",
    "    part4 = (-1)*(0.5*object_dim)* np.log(np.linalg.det(M)) \n",
    "    part5 = (-1/(2*sigma_X**2)) * np.trace(np.dot(np.dot(X.T,(np.identity(num_objects) - np.dot(np.dot(Z,np.linalg.inv(M)),Z.T))),X))\n",
    "    total = part1+part2+part3+part4+part5\n",
    "    return(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Likelihood Functions\n",
    "\n",
    "We now compare our optimized likelihood function likelihood_opt to our unoptimized version to see if we get any significant speed up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original Likelihood</th>\n",
       "      <td>0.000455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Optimized Likelihood</th>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Time\n",
       "Original Likelihood   0.000455\n",
       "Optimized Likelihood  0.000407"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Setting values to use in likelihood function comparisons\n",
    "num_objects = image_data.shape[0]\n",
    "object_dim = image_data.shape[1]\n",
    "\n",
    "sigma_X=1\n",
    "sigma_A=1\n",
    "alpha=1\n",
    "\n",
    "Z, K_plus = sampleIBP(alpha,num_objects)\n",
    "\n",
    "# Time the original likelihood function\n",
    "loops = 1000\n",
    "time_likelihood=np.zeros(loops)\n",
    "for l in range(loops):\n",
    "    t0=time.time()\n",
    "    M = Mcalc(Z, sigma_X, sigma_A, K_plus)\n",
    "    likelihood(image_data, Z, M, sigma_A, sigma_X, K_plus, num_objects, object_dim)\n",
    "    t1=time.time()\n",
    "    time_likelihood[l]=t1-t0\n",
    "mean_time_likelihood= round(np.mean(time_likelihood),7)\n",
    "\n",
    "\n",
    "# Time the optimized likelihood function\n",
    "time_likelihood_opt = np.zeros(loops)\n",
    "for l in range(loops):\n",
    "    t0 = time.time()\n",
    "    likelihood_opt(image_data, Z, sigma_A, sigma_X, K_plus, num_objects, object_dim)\n",
    "    t1 = time.time()\n",
    "    time_likelihood_opt[l] = t1-t0\n",
    "mean_time_likelihood_opt = round(np.mean(time_likelihood_opt), 7)\n",
    "\n",
    "time_array = np.array([mean_time_likelihood, mean_time_likelihood_opt])\n",
    "\n",
    "cols = [\"Time\"]\n",
    "index = [\"Original Likelihood\", \"Optimized Likelihood\"]\n",
    "\n",
    "time_df = pd.DataFrame(time_array, columns = cols, index = index)\n",
    "time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our optimized likelihood function produces a noticeable speed up over our original likelihood function. Over thousands of iterations, this small speed up should accumulate to produce a much faster sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampler_opt Function\n",
    "\n",
    "We rewrite our Sampler function with this new optimization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampler_opt(data, num_objects, object_dim, E=1000,  K_inf = 20, sigma_X = 1, sigma_A = 1, alpha = 1, trunc_val=5):\n",
    "    #Set storage arrays for sampled parameters\n",
    "    chain_Z = np.zeros([E, num_objects, K_inf])\n",
    "    chain_K = np.zeros([E, 1])\n",
    "    chain_sigma_X = np.zeros([E, 1])\n",
    "    chain_sigma_A = np.zeros([E, 1])\n",
    "    chain_alpha = np.zeros([E, 1])\n",
    "\n",
    "    #Initialize parameter values\n",
    "    num_object= np.shape(data)[0]\n",
    "    object_dim = np.shape(data)[1]\n",
    "    [Z, K_plus] = sampleIBP(alpha, num_objects)\n",
    "\n",
    "    #Compute Harmonic Number\n",
    "    HN = 0\n",
    "    for i in range(0, num_objects):\n",
    "        HN = HN + 1.0/(i+1)\n",
    "\n",
    "    for e in range(0, E):\n",
    "        #Store sampled values\n",
    "        chain_Z[e, :, 0:K_plus] = Z[:, 0:K_plus]\n",
    "        chain_K[e] = K_plus\n",
    "        chain_sigma_X[e] = sigma_X\n",
    "        chain_sigma_A[e] = sigma_A\n",
    "        chain_alpha[e] = alpha\n",
    "\n",
    "        if (e%100==0):\n",
    "            print(e)\n",
    "        print(\"At iteration\", e, \": K_plus is\", K_plus, \", alpha is\", alpha) \n",
    "\n",
    "        #Generate a new value for Z[i,k] and accept by Metropolis\n",
    "        for i in range(0, num_objects):\n",
    "            #First we remove singular features if any\n",
    "            for k in range(0, K_plus):\n",
    "                if (k>=K_plus):\n",
    "                    break\n",
    "                if(Z[i, k] > 0):\n",
    "                    if (np.sum(Z[:, k]) - Z[i, k]) <= 0: \n",
    "                        Z[i, k] = 0\n",
    "                        Z[:, k:(K_plus - 1)] = Z[:, (k+1):K_plus]\n",
    "                        K_plus = K_plus - 1\n",
    "                        Z = Z[:, 0:K_plus]\n",
    "                        continue\n",
    "                #Compute conditional distribution for current cell\n",
    "                Z[i,k] = Met_zval(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim, i, k)\n",
    "\n",
    "\n",
    "            #Sample new dishes by Metropolis\n",
    "            new_dishes = New_dishes(data, Z, sigma_X, sigma_A, K_plus, alpha, num_objects, object_dim, trunc_val,i)\n",
    "\n",
    "            if(new_dishes > 0):\n",
    "                newcol = np.zeros((num_objects, new_dishes))\n",
    "                newcol[i,:] = 1\n",
    "                Z = np.column_stack((Z, newcol))\n",
    "            K_plus = K_plus + new_dishes\n",
    "\n",
    "        #Sample sigma_X and sigma_A through Metropolis\n",
    "        [sigma_X, sigma_A] = Met_sigma(data, Z, sigma_X, sigma_A, K_plus, num_objects, object_dim)\n",
    "        #Sample alpha via Gibbs\n",
    "        alpha = np.random.gamma(1 + K_plus, 1/(1+HN))\n",
    "    \n",
    "    print(\"Complete\")\n",
    "    return list([chain_Z, chain_K, chain_sigma_X, chain_sigma_A, chain_alpha])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
